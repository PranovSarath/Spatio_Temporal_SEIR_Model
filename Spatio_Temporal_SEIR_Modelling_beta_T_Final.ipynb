{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwjEjKVasKQ"
      },
      "source": [
        "# **Important Instructions**\n",
        "\n",
        "**Please read before executing the notebook**\n",
        "\n",
        "**1) This notebook is made for running in the Google Colab environment.**\n",
        "\n",
        "**2) If planning to run the notebook in a an Anaconda environment on your local PC, please make sure that the cells relevant to Google Colab are commented out and that you create a new virtual environment and install all the required packages for running this code**\n",
        "\n",
        "**3) I highly recommend using Colab as it does not require any manual changes to be made. If running on Anaconda, you will have to manually fix the bug in the descartes installation**\n",
        "\n",
        "**4) Please make sure the other required Python notebooks and the python code for fetching COVID-19 data from the Open Data API are also available in the current working directory of the notebook**\n",
        "\n",
        "**5)To use the Google Maps API to fetch the geocoding data, you will have to create a Google cloud account and set up an API and an access key. The access key should be copied and pasted on the origin-destination-matrix.ipynb notebook.**\n"
      ],
      "id": "dEwjEjKVasKQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f6bfffd"
      },
      "source": [
        "# 1. Import standard and other required specialised Python packages"
      ],
      "id": "8f6bfffd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37k9-gH9M1H"
      },
      "source": [
        "## 1.1. Install the required packages in Google Colab"
      ],
      "id": "l37k9-gH9M1H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wpgErSMEF8o",
        "outputId": "9c15dab9-7f85-45af-8515-4982e1f4f5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install shapely\n",
        "!pip install descartes\n",
        "!pip install geopandas\n",
        "!pip install networkx\n",
        "!pip install osmnx\n",
        "!pip install contextily\n",
        "!pip install rasterio\n",
        "!pip install lmfit\n",
        "!pip install imageio"
      ],
      "id": "0wpgErSMEF8o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXewwLo4Iheh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#Print current working directory\n",
        "os.getcwd()"
      ],
      "id": "kXewwLo4Iheh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC_GDp17L8lI"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ],
      "id": "LC_GDp17L8lI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytnRSlQaJ6nu"
      },
      "outputs": [],
      "source": [
        "!pip list -v | grep [Dd]escartes"
      ],
      "id": "ytnRSlQaJ6nu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc1Wu1ex5bGy"
      },
      "source": [
        "Verify the location of the descartes installation and check for 'patch.py' file in the folder"
      ],
      "id": "rc1Wu1ex5bGy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf8eOuUWMSWb"
      },
      "outputs": [],
      "source": [
        "!ls '/usr/local/lib/python3.10/dist-packages/descartes'"
      ],
      "id": "Tf8eOuUWMSWb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehDv_Vh68xLU"
      },
      "source": [
        "### **The descartes module installed using conda or the pip package managers have an inherent error in the patch.py file in the installation directory for the package. We need to fix this error by calling the 'coords' attribute in the t.exterior call.**\n",
        "\n",
        "More details and solution available in the link: https://stackoverflow.com/questions/75287534/indexerror-descartes-polygonpatch-wtih-shapely\n",
        "\n",
        "The below script will take care of this issue."
      ],
      "id": "ehDv_Vh68xLU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSPy11H7ODkV"
      },
      "outputs": [],
      "source": [
        "#Location of 'patch.py' file in the descartes installation.\n",
        "file_path = '/usr/local/lib/python3.10/dist-packages/descartes/patch.py'\n",
        "\n",
        "replace_status_flag = 0\n",
        "\n",
        "# Read the file contents\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Check all the lines if they contain the target code\n",
        "target_code = \"\"\"concatenate([asarray(t.exterior)[:, :2]]\"\"\"\n",
        "for i in range(len(lines)):\n",
        "  #print(f\"{i}: {lines[i]}\")\n",
        "  if target_code in lines[i]:\n",
        "      # Replace the code\n",
        "      replace_status_flag = 1\n",
        "      lines[i] = lines[i].replace(\"t.exterior\", \"t.exterior.coords\")\n",
        "\n",
        "      # Write the modified contents back to the file\n",
        "      with open(file_path, 'w') as file:\n",
        "          file.writelines(lines)\n",
        "      print(\"File modified successfully!\")\n",
        "\n",
        "if not(replace_status_flag):\n",
        "  print(\"Target text not found.\")"
      ],
      "id": "YSPy11H7ODkV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQN1XSM-5qj0"
      },
      "source": [
        "Mount google drive location and change current working directory to the Project folder"
      ],
      "id": "fQN1XSM-5qj0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ELAjYE0QB6L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Dissertation/')"
      ],
      "id": "9ELAjYE0QB6L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a99d438"
      },
      "outputs": [],
      "source": [
        "#Standard and specialised module imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import osmnx as ox\n",
        "from descartes import PolygonPatch\n",
        "from shapely.geometry import Point, LineString, Polygon, MultiPolygon, shape, box\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime as dt\n",
        "from pathlib import Path\n",
        "import rasterio as rio\n",
        "from shapely import wkt\n",
        "from rasterio.mask import mask\n",
        "from rasterio import features\n",
        "import contextily as ctx\n",
        "from geopy.geocoders import Nominatim, GoogleV3\n",
        "from shapely.ops import unary_union\n",
        "import networkx as nx\n",
        "from collections import namedtuple\n",
        "from scipy.integrate import odeint\n",
        "import datetime\n",
        "from requests import get\n",
        "from scipy.optimize import minimize\n",
        "from covid_19_numbers import fn_generateSEIRHD_data, fn_get_COVID19_London_data\n",
        "import lmfit\n",
        "from lmfit.lineshapes import gaussian, lorentzian\n",
        "from lmfit import minimize, Parameters\n",
        "from lmfit import report_fit\n",
        "from scipy.optimize import curve_fit\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import re\n",
        "import imageio.v2 as imageio\n",
        "from matplotlib import rcParams\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib import colormaps as matcmp\n",
        "from tqdm.notebook import tqdm\n",
        "from math import trunc"
      ],
      "id": "0a99d438"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gSkXLiv4m2s"
      },
      "outputs": [],
      "source": [
        "#Set random seed so that results can be duplicated\n",
        "random.seed(260758)"
      ],
      "id": "5gSkXLiv4m2s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec9abd2"
      },
      "source": [
        "# 2. Import the map of required city from OpenStreetMaps"
      ],
      "id": "dec9abd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0d5e14f"
      },
      "outputs": [],
      "source": [
        "#Load Lonndon city from openstreetmap using osmnx api\n",
        "london = ox.geocode_to_gdf(\"London, UK\", which_result=1)\n",
        "london.plot()\n",
        "\n",
        "city_of_london = ox.geocode_to_gdf(\"City of London, UK\", which_result=1)\n",
        "city_of_london.plot()\n",
        "\n",
        "#Concatenate the London Region and the City of London to form single GeoDataFrame.\n",
        "city = pd.concat([london, city_of_london])\n",
        "print(city.crs)\n",
        "city = ox.project_gdf(city)\n",
        "ax = city.plot(fc='grey', ec='none')\n",
        "#_ = ax.axis('off')\n",
        "print(city.crs)"
      ],
      "id": "d0d5e14f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21200391"
      },
      "source": [
        "## 2.1. Save the geographical information as a shapefile"
      ],
      "id": "21200391"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96a02efd"
      },
      "outputs": [],
      "source": [
        "city.to_file('LONDON_shapefile.shp', driver='ESRI Shapefile')"
      ],
      "id": "96a02efd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0a1f1c6"
      },
      "outputs": [],
      "source": [
        "london_geometry = unary_union([city['geometry'].iloc[0], city['geometry'].iloc[1]])\n",
        "print(type(london_geometry))"
      ],
      "id": "b0a1f1c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3d57906"
      },
      "source": [
        "## 2.2. Split the city into equal square grids of any width (12km x 12km)"
      ],
      "id": "e3d57906"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "197ccac5"
      },
      "outputs": [],
      "source": [
        "geometry_grids_split = ox.utils_geo._quadrat_cut_geometry(london_geometry, quadrat_width=12000)\n",
        "print(type(geometry_grids_split))"
      ],
      "id": "197ccac5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a61dfc17"
      },
      "outputs": [],
      "source": [
        "print(geometry_grids_split.geoms[0].representative_point().coords[:][0])"
      ],
      "id": "a61dfc17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fa254a9"
      },
      "source": [
        "## 2.3. Assign ID's to each grid and plot the city"
      ],
      "id": "9fa254a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bd53d17"
      },
      "outputs": [],
      "source": [
        "list_polygons = [p for p in list(geometry_grids_split.geoms)]\n",
        "\n",
        "#Figure out the bounds of the city\n",
        "west, south, east, north = city.unary_union.bounds\n",
        "\n",
        "#Plot the city and grids\n",
        "fig, ax = plt.subplots(figsize=(40,40))\n",
        "for polygon, n in zip(geometry_grids_split.geoms, np.arange(1, len(list_polygons))):\n",
        "    #print(polygon)\n",
        "    p = polygon.representative_point().coords[:][0]\n",
        "    patch = PolygonPatch(polygon, fc='#ffffff', ec='#000000', alpha=0.5, zorder=2)\n",
        "    ax.add_patch(patch)\n",
        "    plt.annotate(text=n, xy=p, horizontalalignment='center', size=15)\n",
        "\n",
        "ax.set_xlim(west, east)\n",
        "ax.set_ylim(south, north)\n",
        "ax.axis('off')"
      ],
      "id": "5bd53d17"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19f6ca58"
      },
      "outputs": [],
      "source": [
        "gdf_polygons = gpd.GeoDataFrame(geometry=list_polygons)\n",
        "gdf_polygons.crs = city.crs\n",
        "print(gdf_polygons.crs)\n",
        "gdf_polygons.head()"
      ],
      "id": "19f6ca58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f207cfb0"
      },
      "source": [
        "## 2.4. Overlay the city on the rest of the map (using contextily)"
      ],
      "id": "f207cfb0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66431667"
      },
      "outputs": [],
      "source": [
        "#ctx uses crs epsg:3857\n",
        "#we need to convert the crs to espg:3857\n",
        "gdf_polygons_3857 = gdf_polygons.to_crs(epsg=3857)\n",
        "gdf_polygons_3857['grid_index'] = gdf_polygons_3857.index\n",
        "west, south, east, north = gdf_polygons_3857.unary_union.bounds\n",
        "\n",
        "ax = gdf_polygons_3857.plot(figsize=(20,20), alpha=0.5, edgecolor='k')\n",
        "for idx, row in gdf_polygons_3857.iterrows():\n",
        "    label = row['grid_index']  # Replace 'grid_index' with the desired column containing labels or information\n",
        "    centroid_coords = row['geometry'].centroid.coords[0]\n",
        "    ax.annotate(text=label, xy=centroid_coords, horizontalalignment='center', size=30)\n",
        "#add the rest of the map in the background\n",
        "ctx.add_basemap(ax, zoom=13) #,source=ctx.providers.Stamen.TonerLite)\n",
        "ax.set_xlim(west, east)\n",
        "ax.set_ylim(south, north)"
      ],
      "id": "66431667"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb889e53"
      },
      "source": [
        "# 3. Obtain the population information for the city"
      ],
      "id": "eb889e53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da4226a"
      },
      "source": [
        "For obtaining the population information for the chosen city (London), I am relying on the Gridded Population of the World Dataset (GPW v4).\n",
        "\n",
        "\"The Gridded Population of the World, Version 4 (GPWv4): Population Count, Revision 11 consists of estimates of human population (number of persons per pixel), consistent with national censuses and population registers, for the years 2000, 2005, 2010, 2015, and 2020. A proportional allocation gridding algorithm, utilizing approximately 13.5 million national and sub-national administrative units, was used to assign population counts to 30 arc-second grid cells. The data files were produced as global rasters at 30 arc-second (~1 km at the equator) resolution. To enable faster global processing, and in support of research communities, the 30 arc-second data were aggregated to 2.5 arc-minute, 15 arc-minute, 30 arc-minute and 1 degree resolutions\" **[1]**\n",
        "\n",
        "\n",
        "In this notebook, I am relying on 30 arc-second data which approximately provides the population for a 1KM x 1KM grid at the equator.\n",
        "\n"
      ],
      "id": "4da4226a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ad5ad0"
      },
      "outputs": [],
      "source": [
        "gpw = rio.open('Important Data/GPW Data/gpw_v4_population_count_rev11_2020_30_sec.tif')\n",
        "\n",
        "#Initialise Variables\n",
        "REGION = \"LONDON\"\n",
        "\n",
        "#gdf_london_region = ox.geocode_to_gdf(\"London, UK\", which_result=1)\n",
        "gdf_london_region = gpd.read_file('LONDON_shapefile.shp')\n",
        "gdf_london_region['geometry'] = unary_union([gdf_london_region['geometry'].iloc[0], gdf_london_region['geometry'].iloc[1]])\n",
        "gdf_london_region = gdf_london_region.head(1)\n",
        "gdf_london_region = gdf_london_region.to_crs('4326')\n",
        "gdf_london_region.plot()\n",
        "\n",
        "box = gdf_london_region.total_bounds\n",
        "gpw_region = gpw.read(1, window=gpw.window(*box))\n",
        "\n",
        "\n",
        "# hide\n",
        "def fn_showGeoDataFrame_WithMask(gpdObject, maskObject):\n",
        "    \"\"\"\n",
        "    This function displays the  GeoDataframe and overlays the mask on top of the GeoDataframe\n",
        "\n",
        "    Inputs:\n",
        "    GeoDataframe: This is the actual GeoDataframe passed as input to the function that needs to be displayed\n",
        "    Mask object: This is the mask that needs to be displayed over the object.\n",
        "\n",
        "    Outputs:\n",
        "    None\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(50, 50))\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    gpdObject.plot(ax=ax, facecolor='olive', edgecolor='white', alpha=0.5)\n",
        "    ax.imshow(maskObject, cmap='RdYlGn_r', extent=box[[0,2,1,3]])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "region_mask, region_mask_tfm = mask(dataset=gpw, shapes=gdf_london_region.geometry, all_touched=True, crop=True, filled=True)\n",
        "region_mask = np.where(region_mask < 0, 0, region_mask).squeeze()\n",
        "\n",
        "\n",
        "fn_showGeoDataFrame_WithMask(gdf_london_region, region_mask)\n",
        "\n",
        "\n",
        "# Save the clipped raster\n",
        "region_meta = gpw.meta\n",
        "region_meta.update(dict(\n",
        "    driver='GTiff',\n",
        "    height=region_mask.shape[0],\n",
        "    width=region_mask.shape[1],\n",
        "    transform=region_mask_tfm\n",
        "))\n",
        "with rio.open(f'{REGION}_gpw_output.tif', 'w', **region_meta) as f: f.write(region_mask, indexes=1)\n",
        "\n",
        "\n",
        "# Path to the clipped raster\n",
        "clipped_raster_path = f'{REGION}_gpw_output.tif'\n",
        "\n",
        "# Read the clipped raster\n",
        "with rio.open(clipped_raster_path) as src:\n",
        "    region_mask = src.read(1)\n",
        "    transform = src.transform\n",
        "\n",
        "# Get shapes (polygons) from the raster\n",
        "shapes = features.shapes(region_mask, transform=transform)\n",
        "\n",
        "# Prepare data for GeoDataFrame\n",
        "data = {\n",
        "    'tile_idx': [],\n",
        "    'tile_population': [],\n",
        "    'geometry': [],\n",
        "    'tile_width': [],\n",
        "    'tile_height': []\n",
        "}\n",
        "\n",
        "for idx, (geometry, value) in enumerate(shapes):\n",
        "    if value != 0:  # Skip polygons with no data (value = 0)\n",
        "        tile_idx = idx + 1\n",
        "        tile_population = int(value)\n",
        "        geometry = shape(geometry)\n",
        "        tile_width, tile_height = geometry.bounds[2] - geometry.bounds[0], geometry.bounds[3] - geometry.bounds[1]\n",
        "\n",
        "        data['tile_idx'].append(tile_idx)\n",
        "        data['tile_population'].append(tile_population)\n",
        "        data['geometry'].append(geometry)\n",
        "        data['tile_width'].append(tile_width)\n",
        "        data['tile_height'].append(tile_height)\n",
        "\n",
        "# Create GeoDataFrame\n",
        "gdf = gpd.GeoDataFrame(data)\n",
        "\n",
        "# Save the GeoDataFrame to a shapefile or other formats if needed\n",
        "output_shapefile_path = f'{REGION}_gpw_output.geojson'\n",
        "#gdf = gdf.to_crs(epsg=3857)\n",
        "gdf.to_file(output_shapefile_path)\n",
        "\n",
        "print(f\"GeoDataFrame saved to {output_shapefile_path}\")"
      ],
      "id": "88ad5ad0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54e70d7e"
      },
      "outputs": [],
      "source": [
        "tiles_gdf = gpd.read_file(f'{REGION}_gpw_output.geojson')\n",
        "tiles_gdf"
      ],
      "id": "54e70d7e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abb1489a"
      },
      "outputs": [],
      "source": [
        "tiles_gdf.plot('tile_population',cmap='RdYlGn_r', legend=True)\n",
        "tiles_gdf.crs"
      ],
      "id": "abb1489a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75419b48"
      },
      "outputs": [],
      "source": [
        "tiles_gdf_mercator = tiles_gdf.to_crs(epsg=3857)\n",
        "tiles_gdf_mercator.to_file('file_ldn.shp', driver='ESRI Shapefile')"
      ],
      "id": "75419b48"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb71c93a"
      },
      "source": [
        "# 4. Obtain the population for each grid in the city (Interpolation using area intercepted by the tiles in the GPW v4 dataset and the Gridded city map)"
      ],
      "id": "bb71c93a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b08b3fdb"
      },
      "outputs": [],
      "source": [
        "# Load the GeoDataFrame from the shapefile\n",
        "gpw_gdf = gpd.read_file('file_ldn.shp')\n",
        "gpw_gdf['total_tile_area'] = gpw_gdf.geometry.area\n",
        "gpw_gdf"
      ],
      "id": "b08b3fdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a407545b"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_indexed = gdf_polygons_3857.copy()\n",
        "#gdf_polygons_3857_indexed['grid_index'] = gdf_polygons_3857_indexed.index\n",
        "gdf_polygons_3857_indexed['total_grid_area'] = gdf_polygons_3857_indexed.geometry.area\n",
        "gdf_polygons_3857_indexed"
      ],
      "id": "a407545b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73319b9d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Perform spatial overlay with intersection to get the intersecting areas between the grids and GPW polygons\n",
        "intersection_gdf = gpd.overlay(gdf_polygons_3857_indexed, gpw_gdf, how='intersection')\n",
        "\n",
        "intersection_gdf['intersection_area'] = intersection_gdf.geometry.area\n",
        "intersection_gdf"
      ],
      "id": "73319b9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71aa818c"
      },
      "source": [
        "Approximate the population for each grid in the city. First we calculate the intersection of the grids and tiles and figure out if a partial or complete intersection has happened.\n",
        "\n",
        "If a complete intersection has happened, then the total population of the tile is assigned to the grid.\n",
        "\n",
        "If there is a partial intersection, the ratio of the area of intersection and the total area of the tile is calculated. The population is divided and assigned to each intersecting grid based on this ratio.\n",
        "\n",
        "In reality, this may not truly represent the intricacies of population distribution. However, this is a simpler approximation to solve the problem of population distribution across the city grids."
      ],
      "id": "71aa818c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42988ba7"
      },
      "outputs": [],
      "source": [
        "# Merge the aggregated population data back to the grid GeoDataFrame\n",
        "grid_with_population_final = gdf_polygons_3857_indexed.merge(intersection_gdf[['intersection_area', 'tile_idx','tile_popul','total_tile_area','grid_index']], on='grid_index', how='left')\n",
        "#grid_with_population_final['population_contribution'] = grid_with_population_final['total_grid_area']\n",
        "grid_with_population_final['isCompleteIntersection'] = np.where(grid_with_population_final['total_tile_area']==grid_with_population_final['intersection_area'], 1, 0)\n",
        "grid_with_population_final['population_ratio'] = np.where(grid_with_population_final['isCompleteIntersection']!=1, round(grid_with_population_final.intersection_area/grid_with_population_final.total_tile_area,8), 1)\n",
        "grid_with_population_final['population_contribution'] = (grid_with_population_final['population_ratio'] * grid_with_population_final['tile_popul']).apply(np.ceil).astype('int')\n",
        "grid_with_population_final"
      ],
      "id": "42988ba7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20d9b3c2"
      },
      "source": [
        "# 4.1. Population per grid aggregation"
      ],
      "id": "20d9b3c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e28369d"
      },
      "outputs": [],
      "source": [
        "grid_with_population_final = grid_with_population_final.groupby(['grid_index'])['population_contribution'].sum().reset_index()\n",
        "\n",
        "gdf_polygons_3857_populated = gdf_polygons_3857_indexed.join(grid_with_population_final[['grid_index', 'population_contribution']], on='grid_index', how='inner', lsuffix='_left')\n",
        "gdf_polygons_3857_populated.population_contribution.sum()"
      ],
      "id": "8e28369d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2563ce96"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated"
      ],
      "id": "2563ce96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f969569"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated['population_contribution'].max()"
      ],
      "id": "1f969569"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b05d285"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated['population_contribution'].sum()"
      ],
      "id": "9b05d285"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f99eab55"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated.sort_values('population_contribution', ascending=False).tail(20)"
      ],
      "id": "f99eab55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d64183f8"
      },
      "source": [
        "# 4.2. Plot the population distribution across the city grids"
      ],
      "id": "d64183f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7adb590a"
      },
      "outputs": [],
      "source": [
        "#ctx uses crs epsg:3857\n",
        "#we need to convert the crs to espg:3857\n",
        "west, south, east, north = gdf_polygons_3857_populated.unary_union.bounds\n",
        "\n",
        "ax = gdf_polygons_3857_populated.plot(column='population_contribution',figsize=(40,40), alpha=0.5, edgecolor='k', cmap='magma', legend=True)\n",
        "for idx, row in gdf_polygons_3857_populated.iterrows():\n",
        "    label = row['grid_index']  # Replace 'grid_index' with the desired column containing labels or information\n",
        "    centroid_coords = row['geometry'].centroid.coords[0]\n",
        "    ax.annotate(text=label, xy=centroid_coords, horizontalalignment='center', size=30)\n",
        "\n",
        "\n",
        "#add the rest of the map in the background\n",
        "ctx.add_basemap(ax, zoom=13)\n",
        "ax.set_xlim(west, east)\n",
        "ax.set_ylim(south, north)"
      ],
      "id": "7adb590a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57b5a7a6"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated.to_file('London_grids_with_population.shp', driver='ESRI Shapefile', crs='3857')"
      ],
      "id": "57b5a7a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9144bfda"
      },
      "outputs": [],
      "source": [
        "type(gdf_polygons_3857_populated)"
      ],
      "id": "9144bfda"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "572abb21"
      },
      "outputs": [],
      "source": [
        "gdf_polygons_3857_populated['population_contribution'].sum()"
      ],
      "id": "572abb21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7c0274"
      },
      "source": [
        "# 5. Calculate the Origin-Destination Matrix to model travel across the city grids"
      ],
      "id": "9c7c0274"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0362b07"
      },
      "source": [
        "For building the Origin-Destination Matrix, we will be using a separate notebook which contains all the different operations required to build the aggregated OD matrix for London."
      ],
      "id": "b0362b07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8537a12e"
      },
      "outputs": [],
      "source": [
        "%run ./origin-destination-matrix.ipynb"
      ],
      "id": "8537a12e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59b7c76b"
      },
      "outputs": [],
      "source": [
        "od_matrix_normalized_weekly.shape"
      ],
      "id": "59b7c76b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63NayrAsHgVC"
      },
      "outputs": [],
      "source": [
        "rcParams['figure.figsize'] = (14,14)\n",
        "sns.heatmap(od_matrix_normalized_weekly[0])"
      ],
      "id": "63NayrAsHgVC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c81f80cd"
      },
      "outputs": [],
      "source": [
        "od_matrix_normalized_weekly"
      ],
      "id": "c81f80cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f60a250"
      },
      "source": [
        "## 5.2. Uber Data"
      ],
      "id": "8f60a250"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50542d51"
      },
      "outputs": [],
      "source": [
        "df_uber = pd.read_csv('Important Data/london-traversals-uber-bike.csv')\n",
        "df_uber['geometry'] = df_uber['wktGeometry'].apply(wkt.loads)\n",
        "df_uber"
      ],
      "id": "50542d51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ac12fd"
      },
      "outputs": [],
      "source": [
        "gdf_uber = gpd.GeoDataFrame(df_uber, crs='epsg:4326')\n",
        "gdf_uber"
      ],
      "id": "a2ac12fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3869c3f4"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract start points for a given Polygon\n",
        "def fn_get_start_points(polygon):\n",
        "    \"\"\"\n",
        "    This function return the coordinates of the start point of the uber trip\n",
        "\n",
        "    Inputs:\n",
        "    polygon: A geospatial Polygon object that maps the route of the trip\n",
        "\n",
        "    Output:\n",
        "    Point object: This point identifies the start of the trip.\n",
        "    \"\"\"\n",
        "    exterior_ring = polygon.exterior\n",
        "    start_point = exterior_ring.coords[0]\n",
        "    return Point(start_point)\n",
        "\n",
        "# Define a function to extract end points for a given Polygon\n",
        "def fn_get_end_points(polygon):\n",
        "    \"\"\"\n",
        "    This function return the coordinates of the start point of the uber trip\n",
        "\n",
        "    Inputs:\n",
        "    polygon: A geospatial Polygon object that maps the route of the trip\n",
        "\n",
        "    Output:\n",
        "    Point object: This point identifies the start of the trip.\n",
        "    \"\"\"\n",
        "    exterior_ring = polygon.exterior\n",
        "    #\n",
        "    end_point = exterior_ring.coords[-2]\n",
        "    return Point(end_point)"
      ],
      "id": "3869c3f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "773a0de6"
      },
      "outputs": [],
      "source": [
        "# Extract start and end points using the 'apply' method\n",
        "gdf_uber['start_point'] = gdf_uber['geometry'].apply(lambda polygon: fn_get_start_points(polygon))\n",
        "gdf_uber['end_point'] = gdf_uber['geometry'].apply(lambda polygon: fn_get_end_points(polygon))\n",
        "gdf_uber"
      ],
      "id": "773a0de6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad64c2dc"
      },
      "outputs": [],
      "source": [
        "gdf_uber_origin = gdf_uber.copy()\n",
        "gdf_uber_origin = gdf_uber_origin.to_crs('3857')\n",
        "\n",
        "gdf_uber_origin['geometry'] = gdf_uber_origin['start_point']\n",
        "gdf_uber_origin = gdf_uber_origin[['hexid', 'dayType', 'traversals', 'geometry','start_point','end_point']]\n",
        "\n",
        "# Perform a spatial join to figure out the grid from which each Uber Trip starts from (Find Origin Grid of each trip)\n",
        "start_join = gpd.sjoin(gdf_uber_origin.to_crs('3857'), gdf_polygons_3857_populated[['geometry', 'grid_index']].to_crs('3857'), how='left', predicate='within')\n",
        "start_join = start_join.dropna(subset=['index_right'])\n",
        "start_join = start_join.to_crs('3857')\n",
        "\n",
        "#Change the Geometry of the DataFrame to be based on the destination location\n",
        "start_join['geometry'] = start_join['end_point']\n",
        "#Perform a spatial join to figure out the grid to which each Uber Trip is travelling to\n",
        "end_join = gpd.sjoin(start_join.to_crs('3857'), gdf_polygons_3857_populated[['geometry', 'grid_index']].to_crs('3857'), how='left', predicate='within', lsuffix='start', rsuffix='end')\n",
        "end_join = end_join.drop(['index_right', 'index_end'], axis=1).copy()"
      ],
      "id": "ad64c2dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSEv7x9ex3qJ"
      },
      "outputs": [],
      "source": [
        "end_join.count()"
      ],
      "id": "OSEv7x9ex3qJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Uu20Yw4DiG"
      },
      "source": [
        "## As can be seen in the below pie-chart, only 0.1% trips out of 186930 trips made in the year are between city grids. The huge majority (99.9%) of trips are intra-grid trips."
      ],
      "id": "p6Uu20Yw4DiG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC6StZFgyeZF"
      },
      "outputs": [],
      "source": [
        "#Create a column in the dataframe to identify if the journey is intra grid or inter grid.\n",
        "end_join['travelType'] = np.where(end_join['grid_index_start']!=end_join['grid_index_end'], 'Inter-Grid', 'Intra-Grid')\n",
        "\n",
        "uber_trip_counts = end_join['travelType'].value_counts()\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(uber_trip_counts, labels=uber_trip_counts.index, autopct='%1.1f%%', startangle=180)\n",
        "plt.title('Distribution of Intra-Grid and Inter-Grid trips')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "id": "lC6StZFgyeZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bee398db"
      },
      "outputs": [],
      "source": [
        "#add the rest of the map in the background\n",
        "#ax = end_join.plot(column='geometry',figsize=(40,40), alpha=0.5, edgecolor='k', legend=True)\n",
        "\n",
        "#ctx.add_basemap(ax, zoom=14)\n",
        "#ax.set_xlim(west, east)\n",
        "#ax.set_ylim(south, north)\n",
        "#plt.show()"
      ],
      "id": "bee398db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2cb5f9e"
      },
      "source": [
        "# 6. Implementing the custom SEIR Model"
      ],
      "id": "f2cb5f9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76c5159a"
      },
      "outputs": [],
      "source": [
        "# Flow is a 2D Matrix of dimensions n x n (i.e. 242 x 242)"
      ],
      "id": "76c5159a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93db8cee"
      },
      "outputs": [],
      "source": [
        "n = od_matrix_normalized_weekly.shape[1]\n",
        "N = gdf_polygons_3857_populated['population_contribution'].sum()\n",
        "\n",
        "#initialInd = [2, 116, 131, 72]\n",
        "initialInd = [10]\n",
        "initial = np.zeros(n)\n",
        "initial[initialInd] = 1"
      ],
      "id": "93db8cee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e147d6fe"
      },
      "outputs": [],
      "source": [
        "od_matrix_normalized_weekly.shape"
      ],
      "id": "e147d6fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f754760b"
      },
      "outputs": [],
      "source": [
        "population_vec = gdf_polygons_3857_populated['population_contribution'].to_numpy(dtype=np.float64).copy()"
      ],
      "id": "f754760b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05155ee1"
      },
      "outputs": [],
      "source": [
        "x = len(gdf_polygons_3857_populated)"
      ],
      "id": "05155ee1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805f7d91"
      },
      "outputs": [],
      "source": [
        "def fn_get_OD_matrix_for_day(timestep):\n",
        "    \"\"\"\n",
        "    This function retrieves the current timestamp and returns the Origin Destination matrix for that day\n",
        "\n",
        "    Inputs:\n",
        "    timestep: The current timestep being evaluated by odeint function\n",
        "\n",
        "    Output:\n",
        "    Numpy 2D array: The OD-Matrix for the current timestep\n",
        "    \"\"\"\n",
        "    #Convert the time to a day of the week (0 - 7 (Mon-Sun))\n",
        "    day = int(timestep) % 7\n",
        "    return od_matrix_normalized_weekly[day]"
      ],
      "id": "805f7d91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1a0552f"
      },
      "outputs": [],
      "source": [
        "def fn_seir_baseline(y, t, beta, sigma, gamma, alpha=1):\n",
        "    \"\"\"\n",
        "    This function represents the ODE's for the baseline SEIRHD model. This model does not account for spatial mobility and hence will be used for\n",
        "    model calibration and validation. 'curve_fitting' will be done on this model.\n",
        "\n",
        "    Inputs:\n",
        "    y: the initial or current status of each compartment in the model.\n",
        "    t: The time interval 't' for which the SEIRHD model spread is to be calculated\n",
        "    beta: Rate of COVID-19 transmission\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    alpha: Strength of intervention\n",
        "\n",
        "    Outputs:\n",
        "    Arr: Array containing the population of S, E, I, R, H, D of the 'n' city grids\n",
        "\n",
        "    \"\"\"\n",
        "    OD_M = fn_get_OD_matrix_for_day(t)\n",
        "    x = OD_M.shape[0]\n",
        "    S, E, I, R = y[:x], y[x:2*x], y[2*x:3*x], y[3*x:]\n",
        "\n",
        "    total_population = round(np.sum(y))\n",
        "    assert total_population == N\n",
        "\n",
        "    dS, dE, dI, dR = np.zeros(x), np.zeros(x), np.zeros(x), np.zeros(x)\n",
        "\n",
        "    for i in range(x):\n",
        "        od_sum = OD_M[i,:].sum()\n",
        "\n",
        "        dS[i] = -alpha * beta * S[i] * I[i]/(S[i]+E[i]+I[i]+R[i])\n",
        "        dE[i] = alpha * beta * S[i] * I[i]/(S[i]+E[i]+I[i]+R[i]) - sigma * E[i]\n",
        "        dI[i] = sigma * E[i] - gamma * I[i]\n",
        "        dR[i] = gamma * I[i]\n",
        "\n",
        "    return np.concatenate([dS, dE, dI, dR])"
      ],
      "id": "f1a0552f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eInqSE2xsUUk"
      },
      "outputs": [],
      "source": [
        "def fn_beta_t(t, beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2):\n",
        "    \"\"\"\n",
        "    This function calculates the time-varying value for beta depending upon the interventions during the period.\n",
        "    Inputs:\n",
        "    t: Current time\n",
        "    beta_initial: Transmission rate during initial spread\n",
        "    beta_lockdown: Transmission rate during lockdown\n",
        "    beta_end: Maximum transmission rate after easing restrictions\n",
        "    k1, t1: Parameters for the first logistic function (lockdown imposition)\n",
        "    k2, t2: Parameters for the second logistic function (easing of restrictions)\n",
        "\n",
        "    Outputs:\n",
        "    beta_t: The value of the transmission parameter at time 't'.\n",
        "    \"\"\"\n",
        "    # Logistic function for lockdown imposition\n",
        "    lockdown_effect = beta_initial - (beta_initial - beta_lockdown) / (1 + np.exp(-k1 * (t - t1)))\n",
        "\n",
        "    # Logistic function for easing of restrictions\n",
        "    easing_effect = beta_lockdown + (beta_end - beta_lockdown) / (1 + np.exp(-k2 * (t - t2)))\n",
        "\n",
        "    # Combine the effects\n",
        "    beta_t = lockdown_effect + easing_effect - beta_lockdown  # Subtract beta_lockdown to avoid double counting\n",
        "\n",
        "    return beta_t\n",
        "\n",
        "\n",
        "#This model will be used for curve_fitting as part of the model calibration and validation processes.\n",
        "#The custom SEIRHD spatial mobility model cannot be used for this purpose as the migration terms in the ODE's for each compartment\n",
        "#make the model very complex and leads to increased run times.\n",
        "def fn_seirhd_baseline(y, t, beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2, sigma, gamma, delta, mu, gamma_H, r_I_to_H, alpha=1):\n",
        "    \"\"\"\n",
        "    This function represents the ODE's for the baseline SEIRHD model. This model does not account for spatial mobility and hence will be used for\n",
        "    model calibration and validation. 'curve_fitting' will be done on this model.\n",
        "\n",
        "    Inputs:\n",
        "    y: the initial or current status of each compartment in the model.\n",
        "    t: The time interval 't' for which the SEIRHD model spread is to be calculated\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "    alpha: Strength of intervention\n",
        "\n",
        "    Outputs:\n",
        "    Arr: Array containing the population of S, E, I, R, H, D of the 'n' city grids\n",
        "\n",
        "    \"\"\"\n",
        "    OD_M = fn_get_OD_matrix_for_day(t)\n",
        "    x_size = OD_M.shape[0]\n",
        "    S, E, I, R, H, D = y[:x_size], y[x_size:2*x_size], y[2*x_size:3*x_size], y[3*x_size:4*x_size], y[4*x_size:5*x_size], y[5*x_size:]\n",
        "\n",
        "    total_population = round(np.sum(y))\n",
        "    assert total_population == N\n",
        "\n",
        "    beta = fn_beta_t(t, beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2)\n",
        "\n",
        "    dS, dE, dI, dR, dH, dD = np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size)\n",
        "\n",
        "    for i in range(x_size):\n",
        "        N_i = (S[i]+E[i]+I[i]+R[i]+H[i]+D[i])\n",
        "\n",
        "        dS[i] = -alpha * beta * S[i] * I[i]/N_i\n",
        "        dE[i] = alpha * beta * S[i] * I[i]/N_i - sigma * E[i]\n",
        "        dI[i] = sigma * E[i] - (gamma*(1-r_I_to_H) + delta*r_I_to_H) * I[i]\n",
        "        dH[i] = delta * r_I_to_H * I[i] - (gamma_H + mu) * H[i]\n",
        "        dR[i] = gamma*(1-r_I_to_H) * I[i] + gamma_H * H[i]\n",
        "        dD[i] = mu * H[i]\n",
        "\n",
        "    return np.concatenate([dS, dE, dI, dR, dH, dD])"
      ],
      "id": "eInqSE2xsUUk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tOLbqYMsf8R"
      },
      "outputs": [],
      "source": [
        "#Actual SEIRHD spatial mobility model\n",
        "def fn_seirhd_mobility_model(y, t,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2, sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H, alpha=1):\n",
        "    \"\"\"\n",
        "    This function represents the ODE's for the baseline SEIRHD model. This model does not account for spatial mobility and hence will be used for\n",
        "    model calibration and validation. 'curve_fitting' will be done on this model.\n",
        "\n",
        "    Inputs:\n",
        "    y: the initial or current status of each compartment in the model.\n",
        "    t: The time interval 't' for which the SEIRHD model spread is to be calculated\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    rho: Rate of mobility\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "    alpha: Strength of intervention\n",
        "\n",
        "    Outputs:\n",
        "    Arr: Array containing the population of S, E, I, R, H, D of the 'n' city grids\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    OD_M = fn_get_OD_matrix_for_day(t) * rho\n",
        "    x_size = OD_M.shape[0]\n",
        "    S, E, I, R, H, D = y[:x_size], y[x_size:2*x_size], y[2*x_size:3*x_size], y[3*x_size:4*x_size], y[4*x_size:5*x_size], y[5*x_size:]\n",
        "\n",
        "    total_population = round(np.sum(y))\n",
        "    assert total_population == N\n",
        "\n",
        "    dS, dE, dI, dR, dH, dD = np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size), np.zeros(x_size)\n",
        "\n",
        "    beta = fn_beta_t(t, beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2)\n",
        "\n",
        "    for i in range(x_size):\n",
        "        od_sum = OD_M[i,:].sum()\n",
        "        migration_s = sum([OD_M[j][i] * S[j] for j in range(x_size)]) - S[i] * od_sum\n",
        "        migration_e = sum([OD_M[j][i] * E[j] for j in range(x_size)]) - E[i] * od_sum\n",
        "        migration_i = sum([OD_M[j][i] * I[j] for j in range(x_size)]) - I[i] * od_sum\n",
        "        migration_r = sum([OD_M[j][i] * R[j] for j in range(x_size)]) - R[i] * od_sum\n",
        "\n",
        "        N_i = (S[i]+E[i]+I[i]+R[i]+H[i]+D[i])\n",
        "\n",
        "        dS[i] = -alpha * beta * S[i] * I[i]/N_i + migration_s\n",
        "        dE[i] = alpha * beta * S[i] * I[i]/N_i - sigma * E[i] + migration_e\n",
        "        dI[i] = sigma * E[i] - (gamma*(1-r_I_to_H) + delta*r_I_to_H) * I[i] + migration_i\n",
        "        dH[i] = delta * r_I_to_H * I[i] - (gamma_H + mu) * H[i]\n",
        "        dR[i] = gamma*(1-r_I_to_H) * I[i] + gamma_H * H[i] + migration_r\n",
        "        dD[i] = mu * H[i]\n",
        "\n",
        "    return np.concatenate([dS, dE, dI, dR, dH, dD])"
      ],
      "id": "2tOLbqYMsf8R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um0G3IFwshAg"
      },
      "outputs": [],
      "source": [
        "def fn_setInitialConditions_baseline(df_pop, initial_inf, initial_inf_index, initial_exp=0):\n",
        "    \"\"\"\n",
        "    This function sets the creates the initial conditions required for the baseline SEIR model.\n",
        "    Input Parameters:\n",
        "    df_pop: Geopandas dataframe containing the polygons and the population information of the city grids\n",
        "    initial_inf: The number of initial infections to be seeded in the dataset\n",
        "    initial_inf_index: The index of the grids where the initial infections should be seeded\n",
        "    initial_exp: The number of exposed individuals in the initial stage.\n",
        "\n",
        "    Outputs:\n",
        "    S0, E0, I0, R0 : The initial values of each compartment for every grid in the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    vec_size = len(df_pop)\n",
        "    I0 = [0 if i not in initial_inf_index else initial_inf for i in range(vec_size)]\n",
        "    E0 = [0 if i not in initial_inf_index else initial_exp for i in range(vec_size)]\n",
        "    S0 = [x-(y+z) for x, y, z in zip(df_pop['population_contribution'].tolist(), I0, E0)]\n",
        "    R0 = [0 for _ in range(vec_size)]\n",
        "\n",
        "\n",
        "    return S0, E0, I0, R0\n",
        "\n",
        "S0, E0, I0 , R0= fn_setInitialConditions_baseline(gdf_polygons_3857_populated, initial_inf=1, initial_inf_index=initialInd, initial_exp=100 )\n",
        "y0 = np.concatenate([S0, E0, I0, R0])\n",
        "t = np.linspace(0, 60-1, 60)\n",
        "\n"
      ],
      "id": "Um0G3IFwshAg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69e5f7aa"
      },
      "outputs": [],
      "source": [
        "def fn_setInitialConditions(df_pop, initial_inf, initial_inf_index, initial_exp=0):\n",
        "    \"\"\"\n",
        "    This function sets the creates the initial conditions required for the SEIRHD model.\n",
        "    Input Parameters:\n",
        "    df_pop: Geopandas dataframe containing the polygons and the population information of the city grids\n",
        "    initial_inf: The number of initial infections to be seeded in the dataset\n",
        "    initial_inf_index: The index of the grids where the initial infections should be seeded\n",
        "    initial_exp: The number of exposed individuals in the initial stage.\n",
        "\n",
        "    Outputs:\n",
        "    S0, E0, I0, R0 : The initial values of each compartment for every grid in the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    vec_size = len(df_pop)\n",
        "    I0 = [0 if i not in initial_inf_index else initial_inf for i in range(vec_size)]\n",
        "    E0 = [0 if i not in initial_inf_index else initial_exp for i in range(vec_size)]\n",
        "    S0 = [x-(y+z) for x, y, z in zip(df_pop['population_contribution'].tolist(), I0, E0)]\n",
        "    R0 = [0 for _ in range(vec_size)]\n",
        "    H0 = [0 for _ in range(vec_size)]\n",
        "    D0 = [0 for _ in range(vec_size)]\n",
        "\n",
        "    return S0, E0, I0, R0, H0, D0\n",
        "\n",
        "S0, E0, I0 , R0, H0, D0 = fn_setInitialConditions(gdf_polygons_3857_populated, initial_inf=1, initial_inf_index=initialInd, initial_exp=100 )\n",
        "y0 = np.concatenate([S0, E0, I0, R0, H0, D0])\n",
        "\n",
        "\n",
        "t = np.linspace(0, 100-1, 100)"
      ],
      "id": "69e5f7aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "423e2e56"
      },
      "outputs": [],
      "source": [
        "#beta = 5.0 #Transmission Rate\n",
        "beta_initial = 1.0 #Transmission Rate before Lockdown - initial outbreak\n",
        "beta_lockdown = 0.5 #Transmission Rate during lockdown - after imposing of restrictions\n",
        "beta_end = 1.0 #Transmission Rate during gradual lifting of movement restrictions\n",
        "k1 = 0.1 #Steepness of lockdown beta decrease\n",
        "t1 = 20 #Timestamp at which restrictions are imposed\n",
        "k2 = 0.3 #Steepness of beta increase during ease of restrictions\n",
        "t2 = 60 #Timestamp of ease of restrictions\n",
        "sigma = 0.129 #Rate of Exposed individuals becoming infectious\n",
        "gamma = 0.055 #Recovery Rate\n",
        "alpha = 1 #Strength of Lockdown\n",
        "rho = 1 #Mobility Rate\n",
        "delta = 0.2 #Hospitalisation Rate\n",
        "mu = 0.02 #Mortality Rate\n",
        "gamma_H = 0.99 #Hospitalised Recovery Rate\n",
        "r_I_to_H = 0.50 #Probability of people moving from Infected to Hospitalised\n",
        "\n",
        "res_test = odeint(fn_seirhd_mobility_model, y0, t, args=(beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2, sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H))\n",
        "res_test"
      ],
      "id": "423e2e56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ddcfbb6"
      },
      "outputs": [],
      "source": [
        "def fn_plot_SEIRHD(t, result):\n",
        "    \"\"\"\n",
        "    This function plots the outputs of the SEIRHD model\n",
        "\n",
        "    Inputs:\n",
        "    t: The array containing the array of timestamps over which the model is evaluated\n",
        "    result: The array containing the populations of all grids for all compartments of the model\n",
        "\n",
        "    Output:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract S, E, I, R values from the results\n",
        "    S = result[:, 0:x].sum(axis=1)\n",
        "    E = result[:, x:2*x].sum(axis=1)\n",
        "    I = result[:, 2*x:3*x].sum(axis=1)\n",
        "    R = result[:, 3*x:4*x].sum(axis=1)\n",
        "    H = result[:, 4*x:5*x].sum(axis=1)\n",
        "    D = result[:, 5*x:].sum(axis=1)\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Summing over all regions to get the total values\n",
        "    plt.plot(t, S, 'b', label='Susceptible')\n",
        "    plt.plot(t, E, 'y', label='Exposed')\n",
        "    plt.plot(t, I, 'r', label='Infectious')\n",
        "    plt.plot(t, R, 'g', label='Recovered')\n",
        "    plt.plot(t, H, 'blue', label='Hospitalised')\n",
        "    plt.plot(t, D, 'black', label='Dead')\n",
        "\n",
        "    plt.title('SEIR Model with Mobility')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Population')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "fn_plot_SEIRHD(t, res_test)"
      ],
      "id": "6ddcfbb6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad0a3222"
      },
      "outputs": [],
      "source": [
        "S = res_test[:, 0:x]\n",
        "E = res_test[:, x:2*x]\n",
        "I = res_test[:, 2*x:3*x]\n",
        "R = res_test[:, 3*x:4*x]\n",
        "H = res_test[:, 4*x:5*x]\n",
        "D = res_test[:, 5*x:]\n",
        "\n",
        "r_idx = 49\n",
        "N - round(sum([S[r_idx].sum() , E[r_idx].sum() , I[r_idx].sum() , R[r_idx].sum() , H[r_idx].sum() , D[r_idx].sum() ]))"
      ],
      "id": "ad0a3222"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxH7OuPOhgU8"
      },
      "outputs": [],
      "source": [
        "max_E_val = E.max()\n",
        "index_flat = np.argmax(E)\n",
        "index_2d = np.unravel_index(index_flat, E.shape)\n",
        "max_E_index = index_2d\n",
        "\n",
        "E.max() , max_E_index"
      ],
      "id": "wxH7OuPOhgU8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oHVKCpYe6AK"
      },
      "outputs": [],
      "source": [
        "E.shape"
      ],
      "id": "7oHVKCpYe6AK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL1XYxpPbiTT"
      },
      "outputs": [],
      "source": [
        "gdf_spatial_plot = gdf_polygons_3857.copy()\n",
        "gdf_spatial_plot"
      ],
      "id": "OL1XYxpPbiTT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsh97e2MbUK0"
      },
      "source": [
        "# Spatial Visualisation"
      ],
      "id": "wsh97e2MbUK0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpGJFu2LEc6o"
      },
      "source": [
        "## Build a custom matplotlib colormap"
      ],
      "id": "VpGJFu2LEc6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7SfS0H4ECAt"
      },
      "outputs": [],
      "source": [
        "ncolors = 256\n",
        "# get colormap\n",
        "color_array = matcmp['Reds'](range(ncolors))\n",
        "print(color_array.shape)\n",
        "print(color_array)\n",
        "\n",
        "#Array is of shape 256, 4. 256 colors and 4 channels - R, G, B and alpha. Alpha controls transparency\n",
        "#Change alpha values for transparency\n",
        "color_array[:, -1] = np.linspace(0.3, 1, ncolors)\n",
        "\n",
        "map_obj = LinearSegmentedColormap.from_list(name=\"Reds_transparent1\", colors=color_array)\n",
        "\n",
        "#Register the new color map object\n",
        "#plt.register_cmap(cmap=map_obj)\n",
        "matcmp.register(cmap=map_obj)\n",
        "\n",
        "#plot the example data\n",
        "fig, ax = plt.subplots()\n",
        "h = ax.imshow(np.random.rand(100, 100), cmap = \"Reds_transparent1\")\n",
        "plt.colorbar(mappable=h)"
      ],
      "id": "I7SfS0H4ECAt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsrEEj9gF8gH"
      },
      "outputs": [],
      "source": [
        "def fn_trunc_colormap(cmap , minval=0.0, maxval=1.0, n=100):\n",
        "  new_cmap = LinearSegmentedColormap.from_list('trunc({n}), {a:.2f}, {b:.2f}'.format(n=cmap.name, a = minval, b=maxval), cmap(np.linspace(minval, maxval, n\n",
        "                                                                                                                                           )) )\n",
        "  return new_cmap\n",
        "\n",
        "cmap = matcmp['Reds_transparent1']\n",
        "new_cmap = fn_trunc_colormap(cmap, 0.0 , 0.9)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "h = ax.imshow(np.random.rand(100, 100), cmap=new_cmap)\n",
        "plt.colorbar(mappable=h)"
      ],
      "id": "MsrEEj9gF8gH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIU80b4oN9qV"
      },
      "outputs": [],
      "source": [
        "plot_params = {\"axes.labelcolor\": \"slategrey\"}\n",
        "plt.rcParams.update(plot_params)\n",
        "cmap = matcmp['Blues']\n",
        "blues = cmap(200)\n",
        "\n",
        "\n",
        "def fn_sortInOrder(l):\n",
        "  \"\"\"\n",
        "  Sort a given iterable\n",
        "\n",
        "  Inputs:\n",
        "  l: Input to be sorted\n",
        "\n",
        "  Outputs:\n",
        "  sorted iterable\n",
        "  \"\"\"\n",
        "  convert = lambda text: int(text) if text.isdigit() else text\n",
        "  alphanumeric_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "  return sorted(l , key=alphanumeric_key)\n",
        "\n",
        "\n",
        "def fn_createSpatialVisualisationGIF(scenario, location=\"Scenario GIFS\"):\n",
        "  \"\"\"\n",
        "  Function to create the GIF from the specified location and the given scenario\n",
        "  \"\"\"\n",
        "  filenames = os.listdir('Plots/{}'.format(scenario))\n",
        "  filenames = fn_sortInOrder(filenames)\n",
        "  os.makedirs(location, exist_ok=True)\n",
        "  with imageio.get_writer(f'{location}/London_COVID_19_{scenario}.gif', mode='I', duration=100, quality=50) as writer:\n",
        "    for filename in tqdm(filenames):\n",
        "      image = imageio.imread('Plots/{}/{}'.format(scenario,filename))\n",
        "      writer.append_data(image)\n",
        "\n",
        "\n",
        "def fn_createSpatialVisualisation(model_results, time_steps, scenario=\"\", dpi=50):\n",
        "  \"\"\"\n",
        "  This function creates the spatial data visualisation for the given scenario and SEIRHD spatial mobility model.\n",
        "  The function creates a plot which is saved as an image for each timestep under consideration.\n",
        "  Each of these pictures are then combined to form a GIF that shows the disease spread throughout the space.\n",
        "\n",
        "  Inputs:\n",
        "  model_results: This variable holds the results that are obtained after running the model.\n",
        "  time_steps: This is the list of timesteps over which the model has been run.\n",
        "  scenario: This is a string that identifies the scenario which the model is trying to simulate\n",
        "  dpi: This is an integer that provides the DPI (dots per inch) for the plots. A higher DPI means a higher quality plot is obtained and the resolution is better. However, the trade of is in performance.\n",
        "\n",
        "  Outputs:\n",
        "  None\n",
        "  \"\"\"\n",
        "  S_fn = model_results[:, 0:x]\n",
        "  E_fn = model_results[:, x:2*x]\n",
        "  I_fn = model_results[:, 2*x:3*x]\n",
        "  R_fn = model_results[:, 3*x:4*x]\n",
        "  H_fn = model_results[:, 4*x:5*x]\n",
        "  D_fn = model_results[:, 5*x:]\n",
        "\n",
        "  max_E_val = E_fn.max()\n",
        "  index_flat = np.argmax(E_fn)\n",
        "  index_2d = np.unravel_index(index_flat, E_fn.shape)\n",
        "  max_E_index = index_2d\n",
        "\n",
        "  from tqdm.notebook import tqdm\n",
        "\n",
        "  for timestep in tqdm(range(0, len(time_steps))):\n",
        "    Exposed_I = [E_fn[timestep][grid] for grid in range(x)]\n",
        "    gdf_spatial_plot[\"exposed\"] = Exposed_I\n",
        "    gdf_spatial_plot.head(10)\n",
        "\n",
        "    #plot\n",
        "    fig, ax = plt.subplots(figsize=(14,14), dpi=dpi)\n",
        "    plt.title(f'COVID-19 Spread in London. Scenario: {scenario}', fontsize=22)\n",
        "    gdf_spatial_plot.loc[gdf_spatial_plot.index==7, 'exposed'] = max_E_val + 1\n",
        "    gdf_spatial_plot.plot(ax=ax, facecolor='none', edgecolor='gray', alpha=0.5, zorder=2) #, edgewidth=0.5\n",
        "    gdf_spatial_plot.plot(ax=ax, column='exposed', cmap=new_cmap, zorder=3)\n",
        "\n",
        "    #add a background\n",
        "    ctx.add_basemap(ax, attribution=\"\", url=ctx.providers.Stamen.TonerLite, zoom=13, alpha=0.6)\n",
        "    for idx, row in gdf_polygons_3857_populated.iterrows():\n",
        "      label = row['grid_index']  # Replace 'grid_index' with the desired column containing labels or information\n",
        "      centroid_coords = row['geometry'].centroid.coords[0]\n",
        "      ax.annotate(text=label, xy=centroid_coords, horizontalalignment='center', size=20)\n",
        "\n",
        "    ax.set_xlim(west, east)\n",
        "    ax.set_ylim(south, north)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    insetplot_ax = fig.add_axes([0.6, 0.14, 0.37, 0.27])\n",
        "    insetplot_ax.patch.set_alpha(0.5)\n",
        "\n",
        "    #insetplot_ax.plot(S_fn[:timestep].sum(axis=1), label='Susceptible', color=blues, ls='-', lw=1.5, alpha=1)\n",
        "    insetplot_ax.plot(E_fn[:timestep].sum(axis=1), label='Exposed', color='g', ls='-', lw=1.5, alpha=1)\n",
        "    insetplot_ax.plot(I_fn[:timestep].sum(axis=1), label='Infectious', color='r', ls='-', lw=1.5, alpha=1)\n",
        "    insetplot_ax.plot(R_fn[:timestep].sum(axis=1), label='Recovered', color='y', ls='-', lw=1.5, alpha=1)\n",
        "    insetplot_ax.plot(H_fn[:timestep].sum(axis=1), label='Hospitalised', color='b', ls='-', lw=1.5, alpha=1)\n",
        "    insetplot_ax.plot(D_fn[:timestep].sum(axis=1), label='Dead', color='purple', ls='-', lw=1.5, alpha=1)\n",
        "\n",
        "    insetplot_ax.set_ylabel('Population', size=10, alpha=1, rotation=90)\n",
        "    insetplot_ax.set_xlabel('Days', size=10, alpha=1)\n",
        "    insetplot_ax.yaxis.set_label_coords(-0.15, 0.55)\n",
        "\n",
        "    insetplot_ax.tick_params(direction='in', size=10)\n",
        "    insetplot_ax.set_xlim(-4, 120)\n",
        "    insetplot_ax.set_ylim(-1, max(E_fn.sum(axis=1)) + 50)\n",
        "\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    insetplot_ax.grid(alpha=0.4)\n",
        "\n",
        "    insetplot_ax.spines['right'].set_visible(False)\n",
        "    insetplot_ax.spines['top'].set_visible(False)\n",
        "\n",
        "    insetplot_ax.spines['left'].set_color('darkslategrey')\n",
        "    insetplot_ax.spines['bottom'].set_color('darkslategrey')\n",
        "    insetplot_ax.tick_params(axis='x', color='darkslategrey')\n",
        "    insetplot_ax.tick_params(axis='y', color='darkslategrey')\n",
        "    plt.legend(prop={'size':14, 'weight':'light'}, framealpha=0.5)\n",
        "\n",
        "\n",
        "    results_dir = 'Plots/{}'.format(scenario)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    plt.title(f\"London COVID-19 Spread on Day {timestep}\", fontsize=18, color='dimgray')\n",
        "    plt.savefig('Plots/{}/flows_{}.jpg'.format(scenario, timestep), dpi=fig.dpi)\n",
        "    plt.close()\n",
        "\n",
        "  fn_createSpatialVisualisationGIF(scenario)"
      ],
      "id": "ZIU80b4oN9qV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JET_LQ_JNkmf"
      },
      "outputs": [],
      "source": [
        "fn_createSpatialVisualisation(res_test, t, \"test\", 40)"
      ],
      "id": "JET_LQ_JNkmf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2d31aa"
      },
      "source": [
        "# 7. Real World COVID-19 data for London in 2020"
      ],
      "id": "0d2d31aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27858b5b"
      },
      "source": [
        "#### Let's restrict the simulation to about 1 year (365 days) starting from 2020-02-03 (first infected case reported - sample taken).\n",
        "\n",
        "i.e. From 2020-02-03 till 2021-02-03"
      ],
      "id": "27858b5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40c1349d"
      },
      "outputs": [],
      "source": [
        "# Base URL for the API\n",
        "BASE_URL = \"https://api.coronavirus.data.gov.uk/v1/data\"\n",
        "var_start_date = '2020-01-01'\n",
        "var_areaName = 'london'\n",
        "var_areaType = 'region'\n",
        "\n",
        "endpoint = (\n",
        "        f'{BASE_URL}?'\n",
        "        f'filters=areaType={var_areaType};areaName={var_areaName}&;date={var_start_date}\"'\n",
        "        'structure={\"date\":\"date\",\"newCases\": \"newFirstEpisodesBySpecimenDate\", \"cumulativeCases\": \"cumCasesBySpecimenDate\", \"cumulativeDeaths\": \"cumDeaths28DaysByDeathDate\", \"newDeaths\": \"newWeeklyNsoDeathsByRegDate\", \"dailyHospitalised\":\"covidOccupiedMVBeds\"}'\n",
        ")\n",
        "#newCasesBySpecimenDate\n",
        "#newCasesByPublishDate\n",
        "#newFirstEpisodesBySpecimenDate"
      ],
      "id": "40c1349d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97b3da5d"
      },
      "outputs": [],
      "source": [
        "print(N)\n",
        "# Average incubation period for COVID-19 (assumed to be 5 days)\n",
        "p = 5\n",
        "# Duration after which we assume an individual has recovered (e.g., 14 days)\n",
        "recovery_duration = 14\n",
        "\n",
        "api_data = fn_get_COVID19_London_data(endpoint)\n",
        "dates, S_vec, E_vec, I_vec, I_daily , R_vec, H_vec, D_vec, D_daily_vec = fn_generateSEIRHD_data(api_data, N, p , recovery_duration)"
      ],
      "id": "97b3da5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82aaa6cb"
      },
      "outputs": [],
      "source": [
        "df_all_cases_data = pd.DataFrame({'date':dates, 'S': S_vec, 'E':E_vec, 'I_cumulative': I_vec,\n",
        "                                     'I_daily': I_daily, 'R': R_vec, 'H' : H_vec, 'D_cumulative': D_vec,\n",
        "                                     'D_daily': D_daily_vec})\n",
        "df_all_cases_data['date'] = pd.to_datetime(df_all_cases_data['date'])\n",
        "df_all_cases_data.index = df_all_cases_data['date']\n",
        "df_all_cases_data"
      ],
      "id": "82aaa6cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_7TsDxI-UMZ"
      },
      "source": [
        "### Display important metrics on the COVID-19 Metrics Dataset"
      ],
      "id": "K_7TsDxI-UMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL4apOT1myed"
      },
      "outputs": [],
      "source": [
        "df_all_cases_data.describe()"
      ],
      "id": "lL4apOT1myed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPa9xzsV-YnK"
      },
      "source": [
        "### Total Deaths so far due to COVID-19"
      ],
      "id": "sPa9xzsV-YnK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_z1_Ywqou-j"
      },
      "outputs": [],
      "source": [
        "df_all_cases_data['D_cumulative'].max()"
      ],
      "id": "2_z1_Ywqou-j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5OcK31-d2_"
      },
      "source": [
        "### Maximum number of daily infections till date"
      ],
      "id": "8G5OcK31-d2_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWfUw9ganfb9"
      },
      "outputs": [],
      "source": [
        "df_all_cases_data[df_all_cases_data['I_daily']==df_all_cases_data['I_daily'].max()]"
      ],
      "id": "DWfUw9ganfb9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAHjrxxI-iAD"
      },
      "source": [
        "### Maximum number of daily deaths till date"
      ],
      "id": "HAHjrxxI-iAD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AJkvepZocAE"
      },
      "outputs": [],
      "source": [
        "df_all_cases_data[df_all_cases_data['D_daily']==df_all_cases_data['D_daily'].max()]"
      ],
      "id": "_AJkvepZocAE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpMZkKebaeg_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for column in df_all_cases_data.columns:\n",
        "    if column not in ['date', 'S', 'I_cumulative', 'D_cumulative']:\n",
        "        df_all_cases_data[column].plot(label=column)\n",
        "\n",
        "plt.title('All Values Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "tpMZkKebaeg_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e780b9d"
      },
      "outputs": [],
      "source": [
        "dates_n = [i for i, _ in enumerate(dates)]\n",
        "date2idx = {date:i for i, date in enumerate(dates)}\n",
        "idx2date = {i:date for i, date in enumerate(dates)}\n",
        "print(dates_n[:5])"
      ],
      "id": "3e780b9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xdj7kifUgsW"
      },
      "outputs": [],
      "source": [
        "#Lockdown 1\n",
        "lockdown1_s_date = \"2020-03-23\"\n",
        "lockdown1_e_date = \"2020-07-04\"\n",
        "#Lockdown 2\n",
        "lockdown2_s_date = \"2020-11-05\"\n",
        "lockdown2_e_date = \"2020-12-02\"\n",
        "#Lockdown 3\n",
        "lockdown3_s_date = \"2021-01-06\"\n",
        "lockdown3_e_date = \"2021-03-08\"\n",
        "\n",
        "#Timelines of the COVID-19 Infection waves\n",
        "#The day is not accurate\n",
        "#The dates below merely reflect the approx month of COVID-19 infection\n",
        "wave1_s_date = \"2020-03-01\"\n",
        "wave1_peak_date = \"2020-04-01\"\n",
        "\n",
        "wave2_s_date = \"2020-09-01\"\n",
        "wave2_peak_date = \"2021-01-01\"\n",
        "\n",
        "wave3_s_date = \"2021-05-01\"\n",
        "wave3_peak_date = \"2021-07-01\"\n",
        "\n",
        "wave4_s_date = \"2021-12-01\"\n",
        "wave4_peak_date = \"2021-12-28\"\n",
        "\n",
        "\n",
        "s1_s_date = \"2020-02-03\"\n",
        "s1_e_date = \"2020-08-15\"\n",
        "\n",
        "s2_s_date = \"2020-08-15\"\n",
        "s2_e_date = \"2021-05-01\"\n",
        "\n",
        "s3_s_date = \"2021-05-01\"\n",
        "s3_e_date = \"2022-04-01\"\n",
        "\n",
        "s4_s_date = \"2022-04-01\"\n",
        "s4_e_date = \"2022-12-01\"\n",
        "\n",
        "s5_s_date = \"2022-12-01\"\n",
        "s6_e_date = \"2022-07-01\""
      ],
      "id": "3xdj7kifUgsW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd251520"
      },
      "outputs": [],
      "source": [
        "#train on daily data\n",
        "I_cases = I_daily.copy()\n",
        "D_cases = D_daily_vec.copy()\n",
        "\n",
        "df_cases = pd.DataFrame({'date':dates,'infections':I_cases})\n",
        "\n",
        "#Filter out data after the third stage\n",
        "df_cases = df_cases[df_cases['date']<=s3_s_date]\n",
        "\n",
        "df_cases['date'] = pd.to_datetime(df_cases['date'])\n",
        "fig = df_cases.plot( x='date' , y='infections', c='r', label='Infections', figsize=(12,8))\n",
        "ax = plt.gca()\n",
        "plt.xlabel('Date (t)')\n",
        "plt.ylabel(\"Population P(t)\")\n",
        "\n",
        "#First Lockdown\n",
        "y1_value_start = df_cases[df_cases['date']==lockdown1_s_date]['infections'].values[0]\n",
        "y1_value_end = df_cases[df_cases['date']==lockdown1_e_date]['infections'].values[0]\n",
        "ax.annotate('First lockdown Start', (lockdown1_s_date , y1_value_start), xytext=(lockdown1_s_date, y1_value_start + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "ax.annotate('First lockdown End', (lockdown1_e_date , y1_value_end), xytext=(lockdown1_e_date, y1_value_end + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "\n",
        "\n",
        "#Second Lockdown\n",
        "y2_value_start = df_cases[df_cases['date']==lockdown2_s_date]['infections'].values[0]\n",
        "y2_value_end = df_cases[df_cases['date']==lockdown2_e_date]['infections'].values[0]\n",
        "ax.annotate('Second lockdown Start', (lockdown2_s_date , y2_value_start), xytext=(lockdown2_s_date, y2_value_start + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "ax.annotate('Second lockdown End', (lockdown2_e_date , y2_value_end), xytext=(lockdown2_e_date, y2_value_end + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "\n",
        "#Third Lockdown\n",
        "y3_value_start = df_cases[df_cases['date']==lockdown3_s_date]['infections'].values[0]\n",
        "y3_value_end = df_cases[df_cases['date']==lockdown3_e_date]['infections'].values[0]\n",
        "ax.annotate('Third lockdown Start', (lockdown3_s_date , y3_value_start), xytext=(lockdown3_s_date, y3_value_start + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "ax.annotate('Third lockdown End', (lockdown3_e_date , y3_value_end), xytext=(lockdown3_e_date, y3_value_end + 3000),\n",
        "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "            ha='center', rotation=90 )\n",
        "\n",
        "\n",
        "plt.title('COVID-19 Infections in the London, UK region')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cd251520"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1DW17P3x8UV"
      },
      "outputs": [],
      "source": [
        "s1_start = date2idx[s1_s_date]\n",
        "s1_end = date2idx[s1_e_date]\n",
        "\n",
        "wave1_start = date2idx[wave1_s_date]\n",
        "lockdown1_start = date2idx[lockdown1_s_date]\n",
        "wave1_peak = date2idx[wave1_peak_date]\n",
        "lockdown1_end = date2idx[lockdown1_e_date]\n",
        "\n",
        "plt.xticks(rotation=34)\n",
        "plt.xticks([s1_start, wave1_start, lockdown1_start, wave1_peak, lockdown1_end, s1_end], [s1_s_date, wave1_s_date, lockdown1_s_date, wave1_peak_date, lockdown1_e_date, s1_e_date])\n",
        "plt.tight_layout()\n",
        "plt.plot(dates[s1_start:s1_end], I_cases[s1_start:s1_end], '-')\n",
        "plt.xlabel('Date (t)')\n",
        "plt.ylabel(\"Population P(t)\")\n",
        "plt.title('Infections During Stage 1 of the pandemic')\n",
        "plt.show()\n",
        "\n",
        "#Train on daily data\n",
        "yinf_stage1 = I_cases[s1_start:s1_end]\n",
        "ydead_stage1 = D_cases[s1_start:s1_end]\n",
        "xdata_stage1 = dates_n[s1_start:s1_end]\n",
        "yinf_stage1 = np.array(yinf_stage1, dtype='float')\n",
        "ydead_stage1 = np.array(ydead_stage1, dtype='float')\n",
        "xdata_stage1 = np.array(xdata_stage1, dtype='float')"
      ],
      "id": "C1DW17P3x8UV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e11gavgv4AuK"
      },
      "outputs": [],
      "source": [
        "s2_start = date2idx[s2_s_date]\n",
        "s2_end = date2idx[s2_e_date]\n",
        "\n",
        "wave2_start = date2idx[wave2_s_date]\n",
        "lockdown2_start = date2idx[lockdown2_s_date]\n",
        "wave2_peak = date2idx[wave2_peak_date]\n",
        "lockdown2_end = date2idx[lockdown2_e_date]\n",
        "\n",
        "wave3_start = date2idx[wave3_s_date]\n",
        "wave3_peak = date2idx[wave3_peak_date]\n",
        "lockdown3_start = date2idx[lockdown3_s_date]\n",
        "\n",
        "lockdown3_end = date2idx[lockdown3_e_date]\n",
        "\n",
        "plt.xticks(rotation=34)\n",
        "plt.tight_layout()\n",
        "#plt.plot(dates[s2_start:s2_end], I_cases[s2_start:s2_end], '-')\n",
        "\n",
        "x_values = list(range(s2_start, s2_end))\n",
        "plt.plot(x_values, I_cases[s2_start:s2_end], '-')\n",
        "plt.xticks([s2_start, wave2_start, lockdown2_start, wave2_peak, lockdown2_end, s2_end], [s2_s_date, wave2_s_date, lockdown2_s_date, wave2_peak_date, lockdown2_e_date, s2_e_date])\n",
        "plt.xlabel('Date (t)')\n",
        "plt.ylabel(\"Population P(t)\")\n",
        "plt.title('Infections During Stage 2 of the pandemic')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Train on daily data\n",
        "yinf_stage2 = I_cases[s2_start:s2_end]\n",
        "ydead_stage2 = D_cases[s2_start:s2_end]\n",
        "xdata_stage2 = dates_n[s2_start:s2_end]\n",
        "yinf_stage2 = np.array(yinf_stage2, dtype='float')\n",
        "ydead_stage2 = np.array(ydead_stage2, dtype='float')\n",
        "xdata_stage2 = np.array(xdata_stage2, dtype='float')"
      ],
      "id": "e11gavgv4AuK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-kE4yP54LOt"
      },
      "outputs": [],
      "source": [
        "s3_start = date2idx[s3_s_date]\n",
        "s3_end = date2idx[s3_e_date]\n",
        "\n",
        "wave3_start = date2idx[wave3_s_date]\n",
        "lockdown3_start = date2idx[lockdown3_s_date]\n",
        "wave3_peak = date2idx[wave3_peak_date]\n",
        "lockdown3_end = date2idx[lockdown3_e_date]\n",
        "\n",
        "wave4_start = date2idx[wave4_s_date]\n",
        "wave4_peak = date2idx[wave4_peak_date]\n",
        "\n",
        "#Train on daily data\n",
        "yinf_stage3 = I_cases[s3_start:s3_end]\n",
        "ydead_stage3 = D_cases[s3_start:s3_end]\n",
        "xdata_stage3 = dates_n[s3_start:s3_end]\n",
        "yinf_stage3 = np.array(yinf_stage3, dtype='float')\n",
        "ydead_stage3 = np.array(ydead_stage3, dtype='float')\n",
        "xdata_stage3 = np.array(xdata_stage3, dtype='float')\n",
        "\n",
        "plt.xticks(rotation=34)\n",
        "plt.tight_layout()\n",
        "#plt.plot(dates[s3_start:lockdown3_end], I_cases[s3_start:lockdown3_end], '-')\n",
        "x_values = list(range(s3_start, s3_end))\n",
        "plt.plot(x_values, yinf_stage3, '-')\n",
        "plt.xticks([s3_start, wave3_start, wave3_peak,  wave4_start, wave4_peak, s3_end], [s3_s_date,wave3_s_date, wave3_peak_date, wave4_s_date, wave4_peak_date , s3_e_date])\n",
        "plt.xlabel('Date (t)')\n",
        "plt.ylabel(\"Population P(t)\")\n",
        "plt.title('Infections During Stage 3 of the pandemic')\n",
        "plt.show()"
      ],
      "id": "_-kE4yP54LOt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0c8d6dd"
      },
      "outputs": [],
      "source": [
        "times = np.linspace(0, len(xdata_stage1), len(xdata_stage1))\n",
        "S0, E0, I0, R0, H0, D0 = fn_setInitialConditions(gdf_polygons_3857_populated, initial_inf=1, initial_inf_index=initialInd, initial_exp=100 )"
      ],
      "id": "a0c8d6dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTi28A46gCJA"
      },
      "outputs": [],
      "source": [
        "def fn_fitSEIRHD_mobility_combined(xdata,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H ):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    rho: Rate of mobility\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    results_SEIRHD_mobility = odeint(fn_seirhd_mobility_model, y0, xdata, args=( beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H))\n",
        "    I = results_SEIRHD_mobility[:, 2*x:3*x]\n",
        "    D = results_SEIRHD_mobility[:, 5*x:]\n",
        "    I = I.sum(axis=1)\n",
        "    D = D.sum(axis=1)\n",
        "    return np.concatenate([I, D])\n",
        "\n",
        "\n",
        "def fn_fitSEIRHD_baseline_combined(xdata,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, delta, mu, gamma_H, r_I_to_H ):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    results_SEIRHD_mobility = odeint(fn_seirhd_baseline, y0, xdata, args=( beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2, sigma, gamma, delta, mu, gamma_H, r_I_to_H))\n",
        "    I = results_SEIRHD_mobility[:, 2*x:3*x]\n",
        "    D = results_SEIRHD_mobility[:, 5*x:]\n",
        "    I = I.sum(axis=1)\n",
        "    D = D.sum(axis=1)\n",
        "    return np.concatenate([I, D])\n",
        "\n",
        "\n",
        "def fn_fitSEIRHD_Infected(xdata,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H ):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    rho: Rate of mobility\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    I = odeint(fn_seirhd_mobility_model, y0, xdata, args=( beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H))[:, 2*x:3*x]\n",
        "    return I.sum(axis=1)\n",
        "\n",
        "def fn_fitSEIRHD_Dead(xdata,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H , r_I_to_H):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    rho: Rate of mobility\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    D = odeint(fn_seirhd_mobility_model, y0, xdata, args=( beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, rho, delta, mu, gamma_H, r_I_to_H))[:, 5*x:]\n",
        "    return D.sum(axis=1)\n",
        "\n",
        "def fn_fitSEIRHD_Infected_cv(xdata, beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, delta, mu, gamma_H, r_I_to_H ):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    I = odeint(fn_seirhd_baseline, y0, xdata, args=(beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, delta, mu, gamma_H, r_I_to_H))[:, 2*x:3*x]\n",
        "    return I.sum(axis=1)\n",
        "\n",
        "def fn_fitSEIRHD_Dead_cv(xdata,  beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, delta, mu, gamma_H, r_I_to_H ):\n",
        "    \"\"\"\n",
        "    xdata: The array containing the set of timesteps over which the model is to be fit.\n",
        "    beta_initial: Initial value of the transmission parameter - beta before the start of the lockdown\n",
        "    beta_lockdown: Transmission Rate during the lockdown stages.\n",
        "    beta_end: The transmission rate parameter after the lifting of the intervention.\n",
        "    k1: decides the gradient of the curve/ tranision from\n",
        "    t2: timestep when intervention is introduced\n",
        "    sigma: Rate of infection\n",
        "    gamma: Rate of recovery\n",
        "    delta: Rate of hospitalisation\n",
        "    mu: Fatality rate of COVID-19\n",
        "    gamma_H: Rate of recovery for hospitalised individuals\n",
        "    r_I_to_H: Probability of Infected individuals moving to Hospitalised compartment\n",
        "\n",
        "    Outputs:\n",
        "    result: The array containing the population of the Infected and Dead compartments\n",
        "\n",
        "    \"\"\"\n",
        "    D = odeint(fn_seirhd_baseline, y0, xdata, args=( beta_initial, beta_lockdown, beta_end, k1, t1, k2, t2,  sigma, gamma, delta, mu, gamma_H, r_I_to_H))[:, 5*x:]\n",
        "    return D.sum(axis=1)\n"
      ],
      "id": "MTi28A46gCJA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKBqhtpKWVr"
      },
      "source": [
        "## 7.2. Curve Fitting of SEIRHD model without mobility"
      ],
      "id": "QZKBqhtpKWVr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyhEUc5rjWXa"
      },
      "source": [
        "### 7.2.1. First Stage"
      ],
      "id": "HyhEUc5rjWXa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LilFXzKaaWW"
      },
      "outputs": [],
      "source": [
        "alpha_const, rho_const = 1, 1"
      ],
      "id": "-LilFXzKaaWW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN-PApt5BN1p"
      },
      "outputs": [],
      "source": [
        "def fn_moving_average(data, window_size):\n",
        "    \"\"\"\n",
        "    This function calculates the moving average for each element in the array over the specified window_size\n",
        "\n",
        "    Inputs:\n",
        "    data: This is the input array for which the moving average is to be calculated\n",
        "    window_size: the size of the window over which the average is supposed to be calculated\n",
        "\n",
        "    Output:\n",
        "    array: the output array is of same size and contains the moving average for each element with the same size.\n",
        "    \"\"\"\n",
        "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
        "\n",
        "\n",
        "# Function to annotate the intervention points (t1 and t2)\n",
        "def fn_annotate_point(ax, x, y, label):\n",
        "    \"\"\"\n",
        "    This function annotates the given point with the specified label\n",
        "\n",
        "    Inputs:\n",
        "    ax: The axis object of the matplotlib plot to which the annotatation has to be added\n",
        "    x: The x coordinate of the point\n",
        "    y: The y coordinate of the point\n",
        "    label: The text which has to be marked for the given point\n",
        "    \"\"\"\n",
        "    ax.annotate(label, xy=(x, y), xytext=(x, y + 50),  # position of the text (with a slight offset in the y direction)\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "                horizontalalignment='center')\n",
        "\n",
        "def fn_format_xlabel(value, tick_number):\n",
        "    \"\"\"\n",
        "    This function takes the x-axis value and replaces it with appropriate date value\n",
        "\n",
        "    Inputs:\n",
        "    value: This contains the value of the timestep on the axis\n",
        "    tick_number: index of the tick on the axis-axis\n",
        "    \"\"\"\n",
        "    # Convert index to int (in case it's float)\n",
        "    idx = int(value)\n",
        "\n",
        "    # Return the corresponding date or an empty string if the index is out of range\n",
        "    return idx2date.get(idx, '')\n",
        "\n",
        "\n",
        "def fn_fitModelToStage(xdata, yinfected, lowerbounds, upperbounds, init_estimate, stage, windowSize=5, drawPlots = True):\n",
        "  \"\"\"\n",
        "  This function takes the model and fits it to the supplied infected data\n",
        "\n",
        "  Inputs:\n",
        "  xdata: The array containing the timesteps of over which the model is to be fitted\n",
        "  yinfected: The actual observed data to which the model is to be fitted\n",
        "  lowerbounds: The lowerbounds of all the parameters associated with the model\n",
        "  upperbounds: The array containing the upper bounds of all the parameters associated with the model\n",
        "  init_estimate: The array containing all the initial estimates of the parameters in the model\n",
        "  stage: The stage of the pandemic under investigation\n",
        "  windowSize: The window size for calculating the smoothed average of the infected data\n",
        "  drawPlots: Either true or false to indicate if the plot needs to be drawn or not\n",
        "\n",
        "  Outputs:\n",
        "  None\n",
        "  \"\"\"\n",
        "  #Smoothening the observed data for easier fitting\n",
        "  if drawPlots:\n",
        "    yinfected_smoothed = fn_moving_average(yinfected, windowSize)\n",
        "    plt.plot(xdata, yinfected_smoothed, '-', c='r', label='Smoothened Infected')\n",
        "    plt.plot(xdata, yinfected, 'o', c='b', label='Original Infected')\n",
        "    plt.xlabel('Time (days)')\n",
        "    plt.ylabel('Number of cases')\n",
        "    plt.title(f'Actual Infected Curve vs Smoothened for Stage {stage}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  params, covariance = curve_fit(fn_fitSEIRHD_Infected_cv, xdata, yinfected_smoothed, bounds=(lowerbounds, upperbounds),  p0=tuple(init_estimate ))  # Adjust bounds as needed\n",
        "  beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit, sigma_fit,  gamma_fit , delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit = params\n",
        "\n",
        "  if drawPlots:\n",
        "    #Infected data from SEIRHD model with no mobility\n",
        "    infected_no_mobility_model = fn_fitSEIRHD_Infected_cv(xdata, beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit, sigma_fit , gamma_fit, delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit)\n",
        "    #Infected data from spatial SEIRHD model with mobility\n",
        "    infected_with_mobility_spatial_model = fn_fitSEIRHD_Infected( xdata, beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit, sigma_fit,  gamma_fit, rho_const , delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit)\n",
        "\n",
        "    len_train_data = len(xdata)\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    ax1 = plt.subplot(2, 2, 1)\n",
        "    plt.plot(xdata, yinfected, 'o', c='g', label='Observed Infected')\n",
        "    plt.plot(xdata, yinfected_smoothed, '-', c='r', label='Smoothed Infected')\n",
        "    plt.plot(xdata, infected_no_mobility_model , c='b', label='Fitted Infected')\n",
        "    fn_annotate_point(ax1, t1_fit, yinfected_smoothed[int(t1_fit)], f'Interventions Start = {idx2date[int(t1_fit)]}')\n",
        "    if int(t2_fit) < len_train_data + int(xdata[0]):\n",
        "      fn_annotate_point(ax1, t2_fit, yinfected_smoothed[int(t2_fit)], f'Intervention End = {idx2date[int(t2_fit)]}')\n",
        "    # Set the custom formatter for the x-axis\n",
        "    ax1.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    plt.title(f'Curve Fitting using curve_fit - SEIRHD Baseline for Stage {stage}')\n",
        "    plt.axvline(len(xdata))\n",
        "    plt.xlabel('Time (days)')\n",
        "    plt.ylabel('Number of cases')\n",
        "    plt.legend()\n",
        "    #plt.show()\n",
        "\n",
        "    ax2 = plt.subplot(2, 2, 2)\n",
        "    plt.plot(xdata, infected_no_mobility_model, '--', c='r', label='Fitted Infected (no mobility)')\n",
        "    plt.plot(xdata, infected_with_mobility_spatial_model, '-', c='b', label='Fitted Infected (mobility)')\n",
        "    ax2.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    fn_annotate_point(ax2, t1_fit, yinfected_smoothed[int(t1_fit)], f'Interventions Start = {idx2date[int(t1_fit)]}')\n",
        "    if int(t2_fit) < len_train_data + int(xdata[0]):\n",
        "      fn_annotate_point(ax2, t2_fit, yinfected_smoothed[int(t2_fit)], f'Intervention End = {idx2date[int(t2_fit)]}')\n",
        "    plt.plot(xdata, yinfected_smoothed, 'o' , c='y', label='Smoothed Data')\n",
        "    plt.xlabel('Time (days)')\n",
        "    plt.ylabel('Number of cases')\n",
        "    plt.legend()\n",
        "    plt.title(f'Infection Curve Fitted to SEIRHD Models (Baseline and Mobility) for Stage {stage}')\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "    beta_t_list = []\n",
        "    time_iter_list = np.linspace(0, len(xdata)-1, len(xdata)*2)\n",
        "    for i in time_iter_list:\n",
        "      beta_t_list.append(fn_beta_t(i, beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit))\n",
        "    ax3 = plt.subplot(2, 2, 3)\n",
        "    plt.plot(time_iter_list, beta_t_list)\n",
        "    ax3.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "    plt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    plt.title('Varying Beta over time')\n",
        "    plt.xlabel('Time (days)')\n",
        "    plt.ylabel('Beta')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "  return params, covariance\n",
        "\n",
        "\n",
        "\n",
        "def fn_SEIRHD_predictions(xtrain_data, ytrain_data , xvalidation_data, yvalidation_data, params):\n",
        "  \"\"\"\n",
        "  The function takes the takes the training data and the estimated parameters and outputs the predictions of the SEIRHD model without and with mobility for the validation data\n",
        "\n",
        "  Inputs:\n",
        "  xtrain_data: The array containing the time intervals to which the model is to be trained\n",
        "  ytrain_data: The array containing the actual observed data\n",
        "  xvalidation_data: The time interval for the validation dataset\n",
        "  yvalidaiton_data: The actual observed values in the prediction data\n",
        "  params: The array containing the actual estimated parameters for the model\n",
        "\n",
        "  Output:\n",
        "  no_mobility_predictions_only: The array containing the predictions of the baseline SEIRHD model (with no mobility)\n",
        "  mobility_predictions_only: The array containing the predictions of the spatial mobility SEIRHD model\n",
        "  \"\"\"\n",
        "  beta_initial_pred, beta_lockdown_pred, beta_end_pred, k1_pred, t1_pred, k2_pred, t2_pred,  gamma_pred, sigma_pred , delta_pred, mu_pred, gamma_H_pred, r_I_to_H_pred = params\n",
        "  train_len, validation_len = len(xtrain_data) , len(ytrain_data)\n",
        "  xdata_input = np.concatenate([xtrain_data, xvalidation_data])\n",
        "  rho_pred = 1\n",
        "  no_mobility_model_predictions = fn_fitSEIRHD_Infected_cv(xdata_input, beta_initial_pred, beta_lockdown_pred, beta_end_pred, k1_pred, t1_pred, k2_pred, t2_pred,  gamma_pred, sigma_pred , delta_pred, mu_pred, gamma_H_pred, r_I_to_H_pred )\n",
        "  mobility_model_predictions = fn_fitSEIRHD_Infected( xdata_input, beta_initial_pred, beta_lockdown_pred, beta_end_pred, k1_pred, t1_pred, k2_pred, t2_pred,  gamma_pred, sigma_pred, rho_pred , delta_pred, mu_pred, gamma_H_pred, r_I_to_H_pred)\n",
        "\n",
        "  no_mobility_predictions_only = no_mobility_model_predictions[train_len:train_len+validation_len]\n",
        "  mobility_predictions_only = mobility_model_predictions[train_len:train_len+validation_len]\n",
        "\n",
        "  return no_mobility_predictions_only, mobility_predictions_only\n",
        "\n",
        "\n",
        "\n",
        "def fn_getInitialConditionsToNextStage(stage_params, xdata, y0, rho_init=1):\n",
        "  \"\"\"\n",
        "  This function produces the initial conditions for the next stage of a model given the params that fit to the last stage of a model.\n",
        "  This replicates the first stage using the given parameters and uses the compartment populations at the last time as the initial conditions of the next stage\n",
        "\n",
        "  Inputs:\n",
        "  stage_params: The array containing the best fit parameter values of the model\n",
        "  xdata: The array containing the time intervals that weere used to estimate the previous model\n",
        "  y0: The intial conditions during the start of the model\n",
        "  rho_init: The initial mobility parameter. (default is 1)\n",
        "\n",
        "  Outputs:\n",
        "  new_y0: The array containing the new initial conditions for the model.\n",
        "  \"\"\"\n",
        "  beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit,  gamma_fit, sigma_fit , delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit = stage_params\n",
        "  #seirhd_model_fitted = odeint(fn_seirhd_baseline, y0, xdata, args=(beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit,  gamma_fit, sigma_fit , delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit))\n",
        "  seirhd_model_fitted = odeint(fn_seirhd_baseline, y0, xdata, args=(beta_initial_fit, beta_lockdown_fit, beta_end_fit, k1_fit, t1_fit, k2_fit, t2_fit,  gamma_fit, sigma_fit , delta_fit, mu_fit, gamma_H_fit, r_I_to_H_fit))\n",
        "  last_index = -1\n",
        "  S_end = seirhd_model_fitted[last_index, 0:x]\n",
        "  E_end = seirhd_model_fitted[last_index, x:2*x]\n",
        "  I_end = seirhd_model_fitted[last_index, 2*x:3*x]\n",
        "  R_end = seirhd_model_fitted[last_index, 3*x:4*x]\n",
        "  H_end = seirhd_model_fitted[last_index, 4*x:5*x]\n",
        "  D_end = seirhd_model_fitted[last_index, 5*x:]\n",
        "\n",
        "  print(E_end)\n",
        "\n",
        "  new_y0 = np.concatenate([S_end, E_end, I_end, R_end, H_end, D_end])\n",
        "  return new_y0"
      ],
      "id": "TN-PApt5BN1p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hk7sJwabTqy"
      },
      "outputs": [],
      "source": [
        "def fn_time_series_cross_validation(xdata , ydata , initial_train_size, validation_size, fit_function, predict_function, lowerbounds, upperbounds, initial_params, stage, windowSize = 5, drawPlots = False):\n",
        "    \"\"\"\n",
        "    This function implements time series cross validation on the dataset.\n",
        "\n",
        "    xdata: The array containing the time steps for the entire dataset\n",
        "    ydata: The entire infected dataset for the stage\n",
        "    initial_train_size: initial size of the training dataset\n",
        "    validation_size: Size of the validation dataset (this is also the duration of each validation stage)\n",
        "    fit_function: The function that will be used for fitting the data.\n",
        "    predict_function: The function that will be used for gathering predictions\n",
        "    lowerbounds: The lowerbounds of all the parameters associated with the model\n",
        "    upperbounds: The array containing the upper bounds of all the parameters associated with the model\n",
        "    init_estimate: The array containing all the initial estimates of the parameters in the model\n",
        "    stage: The stage of the pandemic under investigation\n",
        "    windowSize: The window size for calculating the smoothed average of the infected data\n",
        "    drawPlots: Either true or false to indicate if the plot needs to be drawn or not\n",
        "\n",
        "    Outputs:\n",
        "    errors_no_mobility: The validation error produced by the baseline SEIRHD model with no mobility\n",
        "    errors_with_mobility: The validation error produced by the SEIRHD model with mobility.\n",
        "\n",
        "    \"\"\"\n",
        "    errors_no_mobility = []\n",
        "    errors_with_mobility = []\n",
        "    train_end = initial_train_size\n",
        "\n",
        "    iter = 0\n",
        "\n",
        "    while train_end + validation_size <= len(ydata):\n",
        "        # Split data into training and validation sets\n",
        "        xtrain_data = xdata[:train_end]\n",
        "        ytrain_data = ydata[:train_end]\n",
        "\n",
        "        print(iter)\n",
        "        iter = iter + 1\n",
        "\n",
        "        xvalidation_data = xdata[train_end:train_end+validation_size]\n",
        "        yvalidation_data = ydata[train_end:train_end+validation_size]\n",
        "\n",
        "        # Fit the model on the training data\n",
        "        params, _ = fit_function(xtrain_data, ytrain_data , lowerbounds, upperbounds, initial_params, stage, windowSize, drawPlots)\n",
        "\n",
        "        # Predict on the validation data\n",
        "        predictions_no_mobility, predictions_with_mobility = predict_function(xtrain_data, ytrain_data , xvalidation_data, yvalidation_data, params)\n",
        "\n",
        "        # Calculate the error (e.g., MAE)\n",
        "        #error_no_mobility = np.mean(np.abs(predictions_no_mobility - yvalidation_data))\n",
        "        #errors_no_mobility.append(error_no_mobility)\n",
        "\n",
        "        #error_with_mobility = np.mean(np.abs(predictions_with_mobility - yvalidation_data))\n",
        "        #errors_with_mobility.append(error_with_mobility)\n",
        "\n",
        "        #Calculate Root Mean Square Error\n",
        "        error_no_mobility = np.sqrt(np.mean((predictions_no_mobility - yvalidation_data)**2))\n",
        "        errors_no_mobility.append(error_no_mobility)\n",
        "\n",
        "        error_with_mobility = np.sqrt(np.mean((predictions_with_mobility - yvalidation_data)**2))\n",
        "        errors_with_mobility.append(error_with_mobility)\n",
        "\n",
        "\n",
        "        # Roll the window forward\n",
        "        train_end += validation_size\n",
        "\n",
        "    return errors_no_mobility, errors_with_mobility"
      ],
      "id": "6hk7sJwabTqy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw5wEqOfIXf5"
      },
      "outputs": [],
      "source": [
        "#lowerbounds_stage1 = [1, 0.5, 2, 0.01, lockdown1_start-5, 0.01, lockdown1_end-5,  0.01, 0.001, 0.01, 0.01, 0.01, 0.01]\n",
        "#upperbounds_stage1 = [10, 15, 11, 1, lockdown1_start+5, 1, lockdown1_end+5 , 0.5, 1, 0.4, 0.4, 1, 1]\n",
        "\n",
        "#lowerbounds_stage1 = [0.01, 0.0, 0.01, 0.01, lockdown1_start-5, 0.01, lockdown1_end-5,  0.01, 0.001, 0.01, 0.01, 0.01, 0.01]\n",
        "#upperbounds_stage1 = [1, 1, 1, 1, lockdown1_start+5, 1, lockdown1_end+5 , 0.5, 1, 0.4, 0.4, 1, 1]\n",
        "#init_estimate_stage1 = [1, 0.01, 1, 0.4,  lockdown1_start, 0.5, lockdown1_end,  0.5, 0.49, 0.39, 0.39, 0.9, 0.01 ]\n",
        "#params_stage1, covariance_stage1 = fn_fitModelToStage(xdata_stage1, yinf_stage1, lowerbounds_stage1, upperbounds_stage1, init_estimate_stage1, 1)"
      ],
      "id": "hw5wEqOfIXf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ79kJxexGGV"
      },
      "source": [
        "### Set the values of the model parameters"
      ],
      "id": "NJ79kJxexGGV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLmR1X3IxFky"
      },
      "outputs": [],
      "source": [
        "#Duration of Infection - D\n",
        "min_D = 6.5\n",
        "max_D = 13.4\n",
        "\n",
        "#Incubation period - T_incub\n",
        "min_T_incub_period = 2\n",
        "max_T_incub_period = 14\n",
        "\n",
        "#Mortality Rate\n",
        "min_mort = 0.01\n",
        "max_mort = 0.2\n",
        "\n",
        "#hospitalisation Rate\n",
        "min_hosp_rate = 0.05\n",
        "max_hosp_rate = 0.30\n",
        "\n",
        "#R0 - Reproductive Number\n",
        "min_R0 = 0.5\n",
        "max_R0 = 10"
      ],
      "id": "HLmR1X3IxFky"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agh4yTKLxFb7"
      },
      "outputs": [],
      "source": [
        "#beta\n",
        "min_beta = 0.01\n",
        "max_beta = 1.0\n",
        "\n",
        "\n",
        "#gamma\n",
        "min_sigma = 1 / max_D #gamma = 1/D or 1/T_inf\n",
        "max_sigma = 1 / min_D\n",
        "\n",
        "#sigma\n",
        "min_gamma = 1/max_T_incub_period #sigma = 1/T_incub\n",
        "max_gamma = 1/min_T_incub_period\n",
        "\n",
        "#delta\n",
        "min_delta = min_hosp_rate\n",
        "max_delta = max_hosp_rate\n",
        "\n",
        "#mu\n",
        "min_mu = 0.0\n",
        "max_mu = 0.2\n",
        "\n",
        "#gamma_H\n",
        "min_gamma_H = 1 / max_D\n",
        "max_gamma_H = 1 / min_D\n",
        "max_gamma_H = 0.99\n",
        "\n",
        "#r_I_to_H\n",
        "min_r_I_to_H = 0.01\n",
        "max_r_I_to_H = 0.5"
      ],
      "id": "Agh4yTKLxFb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlWE8YI15yOU"
      },
      "outputs": [],
      "source": [
        "print(f\"min beta: {min_beta}\\tmax beta: {max_beta}\")\n",
        "print(f\"min gamma: {min_gamma}\\tmax gamma: {max_gamma}\")\n",
        "print(f\"min sigma: {min_sigma}\\tmax sigma: {max_sigma}\")\n",
        "print(f\"min delta: {min_delta}\\tmax delta: {max_delta}\")\n",
        "print(f\"min mu: {min_mu}\\tmax mu: {max_mu}\")\n",
        "print(f\"min gamma_H: {min_gamma_H}\\tmax gamma_H: {max_gamma_H}\")\n",
        "print(f\"min r_I_to_H: {min_r_I_to_H}\\tmax r_I_to_H: {max_r_I_to_H}\")"
      ],
      "id": "PlWE8YI15yOU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG3u-nXacumk"
      },
      "outputs": [],
      "source": [
        "len(xdata_stage1)"
      ],
      "id": "rG3u-nXacumk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbBZv8fAxE9D"
      },
      "outputs": [],
      "source": [
        "#lowerbounds_stage1 = [1, 0.5, 2, 0.01, lockdown1_start-5, 0.01, lockdown1_end-5,  min_sigma, min_gamma, min_delta, min_mu, min_gamma_H, min_r_I_to_H]\n",
        "#upperbounds_stage1 = [10, 15, 11, 1, lockdown1_start+5, 1, lockdown1_end+5 , max_sigma. max_gamma, max_delta, max_mu, max_gamma_H, max_r_I_to_H]\n",
        "\n",
        "lowerbounds_stage1 = [min_beta, min_beta, min_beta, 0.01, lockdown1_start-5, 0.01, lockdown1_end-5, min_sigma, min_gamma, min_delta, min_mu, min_gamma_H, min_r_I_to_H]\n",
        "upperbounds_stage1 = [max_beta, max_beta, max_beta, 1, lockdown1_start+5, 1, lockdown1_end+5 ,max_sigma, max_gamma, max_delta, max_mu, max_gamma_H, max_r_I_to_H]\n",
        "init_estimate_stage1 = [1, 0.1, 1, 0.7,  lockdown1_start, 0.5, lockdown1_end,  0.1, 0.2, 0.07, 0.2, 0.4, 0.01 ]\n",
        "params_stage1, covariance_stage1 = fn_fitModelToStage(xdata_stage1, yinf_stage1, lowerbounds_stage1, upperbounds_stage1, init_estimate_stage1, 1)"
      ],
      "id": "vbBZv8fAxE9D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VibNch34iEvA"
      },
      "outputs": [],
      "source": [
        "no_mobility_err , mobility_err = fn_time_series_cross_validation(xdata_stage1, yinf_stage1, 125, 7 , fn_fitModelToStage, fn_SEIRHD_predictions, lowerbounds_stage1, upperbounds_stage1, init_estimate_stage1, 1, 5, True)"
      ],
      "id": "VibNch34iEvA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5WKdIbZaN7v"
      },
      "outputs": [],
      "source": [
        "print(sum(no_mobility_err) , sum(mobility_err))"
      ],
      "id": "y5WKdIbZaN7v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghLBNWiA84aV"
      },
      "outputs": [],
      "source": [
        "[float(x) for x in params_stage1]"
      ],
      "id": "ghLBNWiA84aV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfwmxvQzNsZi"
      },
      "outputs": [],
      "source": [
        "y0_start_of_pandemic = y0.copy()\n",
        "y0_start_of_pandemic"
      ],
      "id": "pfwmxvQzNsZi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEHzuMMAKOBp"
      },
      "outputs": [],
      "source": [
        "#y0_stage2 = fn_getInitialConditionsToNextStage(params_stage1, xdata_stage1, y0)"
      ],
      "id": "QEHzuMMAKOBp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3VUde2tvLm"
      },
      "outputs": [],
      "source": [
        "#y0 = y0_stage2.copy()"
      ],
      "id": "7l3VUde2tvLm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_tcCPhajlnk"
      },
      "source": [
        "### 7.2.2. Second Stage"
      ],
      "id": "1_tcCPhajlnk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omrH2o9fPiY2"
      },
      "outputs": [],
      "source": [
        "#len(xdata_stage2), len(yinf_stage2)"
      ],
      "id": "omrH2o9fPiY2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAvOvIWKjqEr"
      },
      "outputs": [],
      "source": [
        "#rho_const = 1xdata_stage2\n",
        "#lowerbounds_stage2 = [min_beta, min_beta, min_beta, 0.01, lockdown2_end-10, 0.01, lockdown3_end-10,  min_sigma, min_gamma, min_delta, min_mu, min_gamma_H, min_r_I_to_H]\n",
        "#upperbounds_stage2 = [max_beta, max_beta, max_beta, 1, lockdown2_end+10, 1, lockdown3_end+10 ,max_sigma, max_gamma, max_delta, max_mu, max_gamma_H, max_r_I_to_H]\n",
        "#init_estimate_stage2 = [1, 0.4, 1, 0.8,  lockdown2_end, 0.1, lockdown3_end,   0.5, 0.1, 0.39, 0.39, 0.4, 0.01 ]\n",
        "#params_stage2, covariance_stage2 = fn_fitModelToStage(, yinf_stage2, lowerbounds_stage2, upperbounds_stage2, init_estimate_stage2, 2, 2)"
      ],
      "id": "IAvOvIWKjqEr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fakn96MA6tVT"
      },
      "source": [
        "### 7.2.3. Curve Fitting for the third stage"
      ],
      "id": "Fakn96MA6tVT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz8Cn31e4T18"
      },
      "outputs": [],
      "source": [
        "#y0_stage3 = fn_getInitialConditionsToNextStage(params_stage2, xdata_stage2, y0)"
      ],
      "id": "cz8Cn31e4T18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK3cGODw4fPD"
      },
      "outputs": [],
      "source": [
        "#y0 = y0_stage3.copy()"
      ],
      "id": "rK3cGODw4fPD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85k28lip4ikQ"
      },
      "outputs": [],
      "source": [
        "#en(xdata_stage2), len(yinf_stage2)"
      ],
      "id": "85k28lip4ikQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWbrPXLu4k9m"
      },
      "outputs": [],
      "source": [
        "#rho_const = 1\n",
        "#lowerbounds_stage3 = [min_beta, min_beta, min_beta, lockdown3_start-20, 0.01, lockdown3_end-20,  min_sigma, min_gamma, min_delta, min_mu, min_gamma_H, min_r_I_to_H]\n",
        "#upperbounds_stage3 = [max_beta, max_beta, max_beta, 1, lockdown3_start+30, 1, lockdown3_end+40, max_sigma, max_gamma, max_delta, max_mu, max_gamma_H, max_r_I_to_H]\n",
        "#init_estimate_stage3 = [1, 1, 1, 0.8,  lockdown3_start, 0.1, lockdown3_end,  0.2, 0.19, 0.19, 0.19, 0.9, 0.01 ]\n",
        "#params_stage3, covariance_stage3 = fn_fitModelToStage(xdata_stage3, yinf_stage3, lowerbounds_stage3, upperbounds_stage3, init_estimate_stage3, 2)"
      ],
      "id": "SWbrPXLu4k9m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76P340-mKc7O"
      },
      "source": [
        "## 7.3. Curve Fitting of SEIR model with mobility"
      ],
      "id": "76P340-mKc7O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4S3ASpsKhXK"
      },
      "outputs": [],
      "source": [
        "#params, covariance = curve_fit(fn_fitSEIRHD_Infected, xdata_stage1, yinf_stage1, bounds=([min_beta, min_beta, min_beta, 0.01, lockdown1_start-5, 0.01, lockdown1_end-5, min_sigma.  min_gamma, rho_const-0.001, min_delta, min_mu, min_gamma_H, min_r_I_to_H], [max_beta, max_beta, max_beta, 1, lockdown1_start+5, 1, lockdown1_end+5 ,max_gamma, max_sigma, rho_const, max_delta, max_mu, max_gamma_H, max_r_I_to_H]), p0=(1, 0.1, 1, 0.7,  lockdown1_start, 0.5, lockdown1_end,  0.1, 0.39, 1, 0.39, 0.39, 0.4, 0.01  ))  # Adjust bounds as needed\n",
        "#[float(x) for x in params]"
      ],
      "id": "W4S3ASpsKhXK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTzDYgYxLJwr"
      },
      "outputs": [],
      "source": [
        "#plt.plot(xdata_stage1, yinf_stage1, 'o', label='Observed Infected')\n",
        "#plt.plot(xdata_stage1, fn_fitSEIRHD_Infected( xdata_stage1, beta_start_est, x0_est, k_est, beta_end_est, sigma_est, gamma_est, rho_const, delta_est, mu_est, gamma_H_est, r_I_to_H_est), label='Fitted Infected')\n",
        "#plt.plot(xdata_stage1, ydead_stage1, '-', label='Observed Dead')\n",
        "#plt.plot(xdata_stage1, fn_fitSEIRHD_Dead( xdata_stage1, beta_start_est, x0_est, k_est, beta_end_est, sigma_est, gamma_est, rho_const, delta_est, mu_est, gamma_H_est, r_I_to_H_est), label='Fitted Dead')\n",
        "#plt.title('Curve Fitting for SEIRHD Model with mobility using curve_fit')\n",
        "#plt.axvline(len(xdata_stage1))\n",
        "#plt.legend()\n",
        "#plt.show()"
      ],
      "id": "dTzDYgYxLJwr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg-YWynpYYkW"
      },
      "source": [
        "# 8. Sensitvity Analysis"
      ],
      "id": "Cg-YWynpYYkW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-BowiiBD8TW"
      },
      "source": [
        "## 8.1. Stage 1 of the pandemic"
      ],
      "id": "3-BowiiBD8TW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eufSnr1tdua6"
      },
      "outputs": [],
      "source": [
        "beta_initial_baseline, beta_lockdown_baseline, beta_end_baseline, k1_baseline, t1_baseline, k2_baseline, t2_baseline,  sigma_baseline, gamma_baseline , delta_baseline, mu_baseline, gamma_H_baseline, r_I_to_H_baseline = params_stage1\n",
        "rho_baseline, alpha_baseline = 1, 1"
      ],
      "id": "eufSnr1tdua6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tl1QoWndfIR"
      },
      "outputs": [],
      "source": [
        "# Baseline parameters\n",
        "params_baseline = {\n",
        "    'beta_initial': beta_initial_baseline,\n",
        "    'beta_lockdown': beta_lockdown_baseline,\n",
        "    'beta_end': beta_end_baseline,\n",
        "    'k1': k1_baseline,\n",
        "    't1': t1_baseline,\n",
        "    'k2': k2_baseline,\n",
        "    't2': t2_baseline,\n",
        "    'sigma': sigma_baseline,\n",
        "    'gamma': gamma_baseline,\n",
        "    'rho': 1.0,\n",
        "    'delta': delta_baseline,\n",
        "    'mu': mu_baseline,\n",
        "    'gamma_H': gamma_H_baseline,\n",
        "    'r_I_to_H': r_I_to_H_baseline,\n",
        "    'alpha': alpha_baseline\n",
        "}\n",
        "params_baseline"
      ],
      "id": "6tl1QoWndfIR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k4F0O8ogC_y"
      },
      "outputs": [],
      "source": [
        "params_min_bounds = {\n",
        "    'beta_initial': min_beta,\n",
        "    'beta_lockdown': min_beta,\n",
        "    'beta_end': min_beta,\n",
        "    'k1': 0.0,\n",
        "    't1': 0,\n",
        "    'k2': 0.0,\n",
        "    't2': 1,\n",
        "    'sigma': 0.0,\n",
        "    'gamma': 0.0,\n",
        "    'rho': 0.0,\n",
        "    'delta': 0.0,\n",
        "    'mu': 0.0,\n",
        "    'gamma_H': 0.0,\n",
        "    'r_I_to_H': 0.0,\n",
        "    'alpha': 0.01\n",
        "}"
      ],
      "id": "7k4F0O8ogC_y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZoiofkBgg-g"
      },
      "outputs": [],
      "source": [
        "params_max_bounds = {\n",
        "    'beta_initial': max_beta,\n",
        "    'beta_lockdown': max_beta,\n",
        "    'beta_end': max_beta,\n",
        "    'k1': 1.0,\n",
        "    't1': len(yinf_stage1)-1,\n",
        "    'k2': 1.0,\n",
        "    't2': len(yinf_stage1),\n",
        "    'sigma': 1.0,\n",
        "    'gamma': 1.0,\n",
        "    'rho': 1.0,\n",
        "    'delta': 1.0,\n",
        "    'mu': 1.0,\n",
        "    'gamma_H': 1.0,\n",
        "    'r_I_to_H': 1.0,\n",
        "    'alpha': 1.0\n",
        "}"
      ],
      "id": "nZoiofkBgg-g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBcNHJz4Elq0"
      },
      "outputs": [],
      "source": [
        "#Set the initial condition\n",
        "y0 = y0_start_of_pandemic.copy()\n",
        "y0"
      ],
      "id": "eBcNHJz4Elq0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM7Yns2DFDhX"
      },
      "outputs": [],
      "source": [
        "params_baseline"
      ],
      "id": "QM7Yns2DFDhX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBb7L5r0YgRc"
      },
      "outputs": [],
      "source": [
        "# Sensitivity analysis\n",
        "for param, baseline_value in params_baseline.items():\n",
        "    # Vary the parameter from its minimum to maximum value with 20 intervals\n",
        "    values = np.linspace(params_min_bounds[param], params_max_bounds[param], 10)\n",
        "    print(param)\n",
        "    peak_infected = []\n",
        "    iter = 0\n",
        "    for value in values:\n",
        "        iter = iter + 1\n",
        "        print(iter)\n",
        "        params_iter = params_baseline.copy()\n",
        "        params_iter[param] = value\n",
        "\n",
        "        #retrieve infected data from the SEIRHD mobility model results\n",
        "        I = odeint(fn_seirhd_mobility_model, y0, xdata_stage1, args=(tuple(list(params_iter.values()))))[:, 2*x:3*x]\n",
        "        I = I.sum(axis=1)\n",
        "\n",
        "\n",
        "        peak_infected.append(max(I))\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure()\n",
        "    plt.title(f'Sensitivity Analysis (Infected Compartment) for parameter: {param}')\n",
        "    plt.plot(values, peak_infected, label=f'Sensitivity to {param}')\n",
        "    plt.xlabel(f\"Param : {param}\")\n",
        "    plt.ylabel('Peak Infected')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "uBb7L5r0YgRc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rofgxfP-dxLR"
      },
      "source": [
        "# 9. Run simulations of different COVID-19 Non-pharmaceutical Intervention scenarios"
      ],
      "id": "rofgxfP-dxLR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B13HhPxekXW"
      },
      "outputs": [],
      "source": [
        "params_baseline"
      ],
      "id": "8B13HhPxekXW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKjgV_DE8Um7"
      },
      "source": [
        "## 9.1. Actual Interventions (This will serve as the baseline to which all other models will be compared to)"
      ],
      "id": "HKjgV_DE8Um7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFMLOcn38ixk"
      },
      "outputs": [],
      "source": [
        "#Result dictionary\n",
        "res = {}"
      ],
      "id": "uFMLOcn38ixk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzTSdU8NHGW4"
      },
      "source": [
        "### The below chart contains very small numbers for all compartments except the Susceptible compartment. Hence we do not see any curves. However the model is working as expected."
      ],
      "id": "vzTSdU8NHGW4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xujDIdJsiLNH"
      },
      "outputs": [],
      "source": [
        "params_actual_intervention = params_baseline.copy()\n",
        "res[\"actual_interventions\"] = odeint(fn_seirhd_mobility_model, y0, xdata_stage1, args=(tuple(list(params_actual_intervention.values()))))\n",
        "fn_plot_SEIRHD(xdata_stage1, res[\"actual_interventions\"])"
      ],
      "id": "xujDIdJsiLNH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR3lSjJDG9vI"
      },
      "source": [
        "### Necessary function definitions for running the scenario simulations"
      ],
      "id": "uR3lSjJDG9vI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84wls0f3Qo-Z"
      },
      "outputs": [],
      "source": [
        "def fn_calculate_doubling_time(data):\n",
        "    \"\"\"\n",
        "    Function to calculate the doubling time for a given dataset.\n",
        "\n",
        "    Inputs:\n",
        "    data: The array containing the dataset for which doubling time is to be calculated.\n",
        "\n",
        "    Outputs:\n",
        "    Doubling time for the dataset.\n",
        "    \"\"\"\n",
        "    doubling_times = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data[i-1] > 0 and data[i] > data[i-1]:\n",
        "            T_d = np.log(2) / (np.log(data[i] / data[i-1]))\n",
        "            doubling_times.append(T_d)\n",
        "        else:\n",
        "            doubling_times.append(np.nan)\n",
        "    return round(np.nanmean(doubling_times),2)\n",
        "\n",
        "\n",
        "\n",
        "def fn_calculate_statistics(data, xdata):\n",
        "    \"\"\"\n",
        "    This function is used to calculate the important metrics for the given dataset.\n",
        "\n",
        "    Inputs:\n",
        "    data: The array containing the dataset produced for each scenario\n",
        "    xdata: The array containing the time intervals of the stage being tested\n",
        "\n",
        "    Outputs:\n",
        "    A dictionary containing the metrics for the input dataset.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'Peak Value': round(np.max(data), 2),\n",
        "        'Time to Peak': xdata[np.argmax(data)],\n",
        "        'Total Value at End': round(data[-1], 2),\n",
        "        'Doubling Time': fn_calculate_doubling_time(data)\n",
        "    }\n",
        "\n",
        "\n",
        "# Function to annotate the intervention points (t1 and t2)\n",
        "def fn_annotate_point(ax, x, y, label):\n",
        "    \"\"\"\n",
        "    This function annotates the given point with the specified label\n",
        "\n",
        "    Inputs:\n",
        "    ax: The axis object of the matplotlib plot to which the annotatation has to be added\n",
        "    x: The x coordinate of the point\n",
        "    y: The y coordinate of the point\n",
        "    label: The text which has to be marked for the given point\n",
        "    \"\"\"\n",
        "    ax.annotate(label, xy=(x, y), xytext=(x, y+4),  # position of the text (with a slight offset in the y direction)\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
        "                horizontalalignment='center')\n",
        "\n",
        "\n",
        "def fn_annotate_interventions(ax, arr_scenario, t1, t2, xdata, len_xdata, intervention_lines=True):\n",
        "  \"\"\"\n",
        "  Function to annotate the interventions on the Baseline vs 'Simulated Scenario' plots\n",
        "\n",
        "  Inputs:\n",
        "  ax: Axis of the plot to which the annotations need to be added\n",
        "  arr_scenario: The relavant array of the compartment being plotted\n",
        "  t1: Timestamp of the start of the intervention\n",
        "  t2: Timestamp of the end of the intervention\n",
        "  xdata: Array containing the timestamps for which the results are generated\n",
        "  len_xdata: The length of the timestamp array (xdata)\n",
        "  intervention_lines: Boolean value identifying if the plot requires the annotations to be plotted or not\n",
        "\n",
        "  Outputs:\n",
        "  None\n",
        "  \"\"\"\n",
        "  if intervention_lines:\n",
        "    ax.axvline(t1, linestyle='--', c='r', label='Intervention start')\n",
        "    fn_annotate_point(ax, t1, arr_scenario[t1], f\"Interventions Start = {idx2date[t1]}\")\n",
        "    if t2 < len_xdata + int(xdata[0]):\n",
        "      if intervention_lines:\n",
        "        ax.axvline(t2, linestyle='--', c='g', label='Intervention end')\n",
        "        fn_annotate_point(ax, t2, arr_scenario[t2], f\"Intervention End = {idx2date[t2]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fn_plot_Baseline_VS_Scenarios(baseline_model_outputs, xdata, y_initial, params_dict, scenario, y_scale='linear', intervention_lines=True, baseline_scenario='Baseline'):\n",
        "  \"\"\"\n",
        "  This function plots the outputs of the current scenario being modelled against the chosen baseline model\n",
        "  baseline_model_outputs: The array containing the outputs of the baseline model (already calculated using odeint)\n",
        "  xdata: The array containing the time intervals of the stage being tested\n",
        "  y_initial: The initial conditions of the stage under consideration\n",
        "  params_dict: The dictionary contains the estimated values for each parameter of the model\n",
        "  scenario: The name of the scenario being tested\n",
        "  y_scale: The scale of the y-axis for the graphs being plotted\n",
        "  intervention_lines: A boolean value indicating if the time of intervention should be annotated on the plots\n",
        "  baseline_scenario: The name of the baseline scenario.\n",
        "\n",
        "  Outputs:\n",
        "  None\n",
        "\n",
        "  \"\"\"\n",
        "  S_baseline = baseline_model_outputs[:, 0:x].sum(axis=1)\n",
        "  E_baseline = baseline_model_outputs[:, x:2*x].sum(axis=1)\n",
        "  I_baseline = baseline_model_outputs[:, 2*x:3*x].sum(axis=1)\n",
        "  R_baseline = baseline_model_outputs[:, 3*x:4*x].sum(axis=1)\n",
        "  H_baseline = baseline_model_outputs[:, 4*x:5*x].sum(axis=1)\n",
        "  D_baseline = baseline_model_outputs[:, 5*x:].sum(axis=1)\n",
        "\n",
        "\n",
        "  scenario_model_outputs = odeint(fn_seirhd_mobility_model, y_initial, xdata, args=(tuple(list(params_dict.values()))))\n",
        "  S_scenario = scenario_model_outputs[:, 0:x].sum(axis=1)\n",
        "  E_scenario = scenario_model_outputs[:, x:2*x].sum(axis=1)\n",
        "  I_scenario = scenario_model_outputs[:, 2*x:3*x].sum(axis=1)\n",
        "  R_scenario = scenario_model_outputs[:, 3*x:4*x].sum(axis=1)\n",
        "  H_scenario = scenario_model_outputs[:, 4*x:5*x].sum(axis=1)\n",
        "  D_scenario = scenario_model_outputs[:, 5*x:].sum(axis=1)\n",
        "\n",
        "  len_train_data = len(xdata)\n",
        "\n",
        "\n",
        "  #Plotting S, E, I, R, D in different compartments\n",
        "\n",
        "  #Susceptible Compartment\n",
        "  plt.figure(figsize=(30, 30))\n",
        "  ax1 = plt.subplot(3, 2, 1)\n",
        "  plt.plot(xdata, S_baseline, 'o', linewidth=10, c='b' , label=f'{baseline_scenario} Susceptible')\n",
        "  plt.plot(xdata, S_scenario , linewidth=10, c='r', label=f'{scenario} Susceptible')\n",
        "  fn_annotate_interventions(ax1, S_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax1.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax1.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Susceptible Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  #Exposed Compartment\n",
        "  ax2 = plt.subplot(3, 2, 2)\n",
        "  plt.plot(xdata, E_baseline, 'o',  linewidth=10, c='b' , label=f'{baseline_scenario} Exposed')\n",
        "  plt.plot(xdata, E_scenario , linewidth=10, c='r' , label=f'{scenario} Exposed')\n",
        "  fn_annotate_interventions(ax2, E_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax2.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax2.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Exposed Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  #Infected Compartment\n",
        "  ax3 = plt.subplot(3, 2, 3)\n",
        "  plt.plot(xdata, I_baseline, 'o',  linewidth=10, c='b' , label=f'{baseline_scenario} Infected')\n",
        "  plt.plot(xdata, I_scenario, linewidth=10, c='r' , label=f'{scenario} Infected')\n",
        "  fn_annotate_interventions(ax3, I_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax3.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax3.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax3.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Infected Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  #Recovered Compartment\n",
        "  ax4 = plt.subplot(3, 2, 4)\n",
        "  plt.plot(xdata, R_baseline, 'o',  linewidth=10, c='b' , label=f'{baseline_scenario} Recovered')\n",
        "  plt.plot(xdata, R_scenario ,  linewidth=10, c='r' , label=f'{scenario} Recovered')\n",
        "  fn_annotate_interventions(ax4, R_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax4.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax4.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax4.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Recovered Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  #Hospitalised Compartment\n",
        "  ax5 = plt.subplot(3, 2, 5)\n",
        "  plt.plot(xdata, H_baseline, 'o',  linewidth=10, c='b', label=f'{baseline_scenario} Hospitalised')\n",
        "  plt.plot(xdata, H_scenario ,  linewidth=10, c='r', label=f'{scenario} Hospitalised')\n",
        "  fn_annotate_interventions(ax5, H_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax5.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax5.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax5.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Hospitalised Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "  #Dead Compartment\n",
        "  ax6 = plt.subplot(3, 2, 6)\n",
        "  plt.plot(xdata, D_baseline, 'o',  linewidth=10, c='b', label=f'{baseline_scenario} Dead')\n",
        "  plt.plot(xdata, D_scenario ,  linewidth=10, c='r', label=f'{scenario} Dead')\n",
        "  fn_annotate_interventions(ax6, D_scenario, int(params_dict['t1']), int(params_dict['t2']), xdata, len_train_data, intervention_lines=intervention_lines)\n",
        "  ax6.xaxis.set_major_formatter(FuncFormatter(fn_format_xlabel))\n",
        "  plt.setp(ax6.get_xticklabels(), rotation=45, ha=\"right\", weight='bold', fontsize=22)\n",
        "  plt.setp(ax6.get_yticklabels(), weight='bold', fontsize=22)\n",
        "  plt.title(f'Dead Curves - {baseline_scenario} vs. {scenario}', fontsize=22)\n",
        "  plt.xlabel('Time (days)', fontsize=22)\n",
        "  plt.ylabel('Number of cases', fontsize=22)\n",
        "  plt.yscale(y_scale)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  #Plot the tables showing key metrics to measure disease spraead\n",
        "  titles = ['Exposed', 'Infected', 'Hospitalized', 'Dead']\n",
        "  baseline_values = [E_baseline, I_baseline, H_baseline, D_baseline]\n",
        "  scenario_values = [E_scenario, I_scenario, H_scenario, D_scenario]\n",
        "\n",
        "  # Calculate statistics\n",
        "  baseline_stats_data = []\n",
        "  scenario_stats_data = []\n",
        "  for title, baseline, scenario_z in zip(titles, baseline_values, scenario_values):\n",
        "      baseline_stats = fn_calculate_statistics(baseline, xdata)\n",
        "      scenario_stats = fn_calculate_statistics(scenario_z, xdata)\n",
        "\n",
        "      baseline_stats_data.append([title] + list(baseline_stats.values()))\n",
        "      scenario_stats_data.append([title] + list(scenario_stats.values()))\n",
        "\n",
        "  # Convert stats data to DataFrame for easy display\n",
        "  table_columns = ['Metric', f\"Peak Value\", f\"Time to Peak (days)\", f\"Total Value at End\", f\"Doubling Time (days)\"]\n",
        "\n",
        "  baseline_df = pd.DataFrame(baseline_stats_data, columns=table_columns)\n",
        "  scenario_df = pd.DataFrame(scenario_stats_data, columns=table_columns)\n",
        "\n",
        "  # Display statistics tables using subplots\n",
        "  fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
        "\n",
        "  # Baseline table\n",
        "  ax1.axis('off')\n",
        "  table1 = ax1.table(cellText=baseline_df.values, colLabels=baseline_df.columns, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(table_columns))\n",
        "  table1.auto_set_font_size(False)\n",
        "  table1.set_fontsize(10)\n",
        "  table1.scale(1.5, 1.5)\n",
        "  ax1.set_title(f\"Metrics for '{baseline_scenario}' scenario\")\n",
        "\n",
        "  # Scenario table\n",
        "  ax2.axis('off')\n",
        "  table2 = ax2.table(cellText=scenario_df.values, colLabels=scenario_df.columns, cellLoc='center', loc='center', colColours=['#f5f5f5']*len(table_columns))\n",
        "  table2.auto_set_font_size(False)\n",
        "  table2.set_fontsize(10)\n",
        "  table2.scale(1.5, 1.5)\n",
        "  ax2.set_title(f\"Metrics for '{scenario}' scenario\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "id": "84wls0f3Qo-Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymHkRHnJ8qBA"
      },
      "source": [
        "## 9.2. Scenario: No NPI's in effect. Zero interventions."
      ],
      "id": "ymHkRHnJ8qBA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqR4o-oanUrb"
      },
      "outputs": [],
      "source": [
        "params_no_intervention = params_baseline.copy()\n",
        "params_no_intervention['beta_lockdown'] = params_no_intervention['beta_initial']\n",
        "params_no_intervention['beta_end'] = params_no_intervention['beta_initial']\n",
        "params_no_intervention['k1'] = 1\n",
        "params_no_intervention['k2'] = 1\n",
        "params_no_intervention"
      ],
      "id": "FqR4o-oanUrb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWKlQPGw__xK"
      },
      "outputs": [],
      "source": [
        "res[\"actual_interventions_test\"] = odeint(fn_seirhd_mobility_model, y0, xdata_stage1, args=(tuple(list(params_actual_intervention.values()))))\n",
        "fn_createSpatialVisualisation(res[\"actual_interventions_test\"], xdata_stage1, \"Actual Interventions\", 40)"
      ],
      "id": "WWKlQPGw__xK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WUDx-g5eiTu"
      },
      "outputs": [],
      "source": [
        "res[\"no_interventions\"] = odeint(fn_seirhd_mobility_model, y0, xdata_stage1, args=(tuple(list(params_no_intervention.values()))))\n",
        "fn_plot_SEIRHD(xdata_stage1, res[\"no_interventions\"])"
      ],
      "id": "6WUDx-g5eiTu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrenNyByS8di"
      },
      "outputs": [],
      "source": [
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_no_intervention, 'Zero Interventions', 'log', False)"
      ],
      "id": "DrenNyByS8di"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs2N5bOugq8N"
      },
      "source": [
        "## 9.3 Lockdown mandates in place on the estimated day of lockdown start till estimated end date (80% population follows lockdown mandates)"
      ],
      "id": "gs2N5bOugq8N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzUKzPT0hD-a"
      },
      "outputs": [],
      "source": [
        "params_80pc_lockdown = params_baseline.copy()\n",
        "params_80pc_lockdown['rho'] = 0.2\n",
        "params_80pc_lockdown['alpha'] = 0.2\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_80pc_lockdown, '80% Lockdown', 'linear')"
      ],
      "id": "ZzUKzPT0hD-a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiQTWIubiK4l"
      },
      "source": [
        "## 9.4. Scenario - strict Lockdown is announced within 30 days of first reported case. and gradually reopened:"
      ],
      "id": "JiQTWIubiK4l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEtgx2IKgqrX"
      },
      "outputs": [],
      "source": [
        "params_strict_lockdown_smooth_reopening = params_baseline.copy()\n",
        "params_strict_lockdown_smooth_reopening['beta_lockdown'] = 0.10\n",
        "params_strict_lockdown_smooth_reopening['beta_end'] = 0.8\n",
        "params_strict_lockdown_smooth_reopening['k1'] = 0.05\n",
        "params_strict_lockdown_smooth_reopening['t1'] = 25\n",
        "params_strict_lockdown_smooth_reopening['k2'] = 0.99\n",
        "params_strict_lockdown_smooth_reopening['t2'] = 120\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_strict_lockdown_smooth_reopening, 'Strict Lockdown Gradual Reopening', 'linear')"
      ],
      "id": "GEtgx2IKgqrX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts56pvn-uXaw"
      },
      "source": [
        "## 9.5. Scenario - Lockdown is announced strict within 30 days of first reported case and restrictions are not lifted"
      ],
      "id": "ts56pvn-uXaw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxSnFCkngqfa"
      },
      "outputs": [],
      "source": [
        "params_strict_lockdown_no_reopening = params_baseline.copy()\n",
        "params_strict_lockdown_no_reopening['beta_lockdown'] = 0.10\n",
        "params_strict_lockdown_no_reopening['beta_end'] = 0.10\n",
        "params_strict_lockdown_no_reopening['k1'] = 0.05\n",
        "params_strict_lockdown_no_reopening['t1'] = 25\n",
        "params_strict_lockdown_no_reopening['k2'] = 0.99\n",
        "params_strict_lockdown_no_reopening['t2'] = 520\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_strict_lockdown_no_reopening, 'Strict Lockdown No Reopening', 'linear')"
      ],
      "id": "WxSnFCkngqfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC9eD26VxROD"
      },
      "source": [
        "## 9.6. Late Interventions (Lockdowns are not imposed within the first 100 days of infection)"
      ],
      "id": "jC9eD26VxROD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztrp1pW_gqUB"
      },
      "outputs": [],
      "source": [
        "params_strict_late_interventions = params_baseline.copy()\n",
        "params_strict_late_interventions['beta_lockdown'] = 0.10\n",
        "params_strict_late_interventions['beta_end'] = 0.70\n",
        "params_strict_late_interventions['k1'] = 0.05\n",
        "params_strict_late_interventions['t1'] = 100\n",
        "params_strict_late_interventions['k2'] = 0.99\n",
        "params_strict_late_interventions['t2'] = 520\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_strict_late_interventions, 'Very Late Interventions', 'linear')"
      ],
      "id": "Ztrp1pW_gqUB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoo5FecT_Xq_"
      },
      "source": [
        "## 9.7. Social distancing, mask wearing mandates in place and followed by the public"
      ],
      "id": "yoo5FecT_Xq_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3CW1MVFzD1p"
      },
      "outputs": [],
      "source": [
        "params_strict_social_distancing_masks_mandates= params_baseline.copy()\n",
        "params_strict_social_distancing_masks_mandates['beta_lockdown'] = 0.10\n",
        "params_strict_social_distancing_masks_mandates['beta_end'] = 0.40\n",
        "params_strict_social_distancing_masks_mandates['k1'] = 0.05\n",
        "params_strict_social_distancing_masks_mandates['k2'] = 0.55\n",
        "params_strict_social_distancing_masks_mandates['alpha'] = 0.8\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_strict_social_distancing_masks_mandates, 'Social Distancing & Face Masks', 'linear')"
      ],
      "id": "y3CW1MVFzD1p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IY-ceIggag6"
      },
      "source": [
        "## 9.8. Reducing spatial mobility"
      ],
      "id": "0IY-ceIggag6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ingu8iykna94"
      },
      "outputs": [],
      "source": [
        "od_matrix_normalized_weekly_backup = od_matrix_normalized_weekly.copy()"
      ],
      "id": "Ingu8iykna94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymaEh3p3o6-H"
      },
      "outputs": [],
      "source": [
        "od_matrix_normalized_weekly= od_matrix_normalized_weekly_backup.copy()"
      ],
      "id": "ymaEh3p3o6-H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgw_MiLQnvod"
      },
      "outputs": [],
      "source": [
        "grids_to_restrict = [4,5,9,10,13,14]\n",
        "\n",
        "for i in range(od_matrix_normalized_weekly.shape[0]):\n",
        "  for grid in grids_to_restrict:\n",
        "    for columnIdx in range(od_matrix_normalized_weekly.shape[1]):\n",
        "      od_matrix_normalized_weekly[i][grid][columnIdx] = 0\n",
        "      od_matrix_normalized_weekly[i][columnIdx][grid] = 0\n",
        "\n",
        "od_matrix_normalized_weekly"
      ],
      "id": "Lgw_MiLQnvod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPetFZmtpDd-"
      },
      "outputs": [],
      "source": [
        "params_restricted_spatial_movement = params_baseline.copy()\n",
        "params_restricted_spatial_movement['rho'] = 0.4\n",
        "params_restricted_spatial_movement['alpha'] = 0.4\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_restricted_spatial_movement, 'Reduced Spatial Movement', 'linear')"
      ],
      "id": "oPetFZmtpDd-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBfoMKk6_88h"
      },
      "source": [
        "## 9.9 Strict Contact Tracing, Quarantine and Isolation of Infected Individuals"
      ],
      "id": "gBfoMKk6_88h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEo3f4_OgqAM"
      },
      "outputs": [],
      "source": [
        "params_strict_contact_tracing = params_baseline.copy()\n",
        "params_strict_contact_tracing['beta_end'] = 0.40\n",
        "params_strict_contact_tracing['alpha'] = 0.8\n",
        "params_strict_contact_tracing['gamma'] = 0.8\n",
        "params_strict_contact_tracing['delta'] = 0.8\n",
        "params_strict_contact_tracing['r_I_to_H'] = 0.5\n",
        "params_strict_contact_tracing['gamma_H'] = 0.8\n",
        "\n",
        "\n",
        "fn_plot_Baseline_VS_Scenarios(res[\"actual_interventions\"], xdata_stage1, y0, params_strict_contact_tracing, 'Contact Tracing, Isolation & Quarantine', 'linear')"
      ],
      "id": "DEo3f4_OgqAM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "392445d4"
      },
      "source": [
        "# References:\n",
        "\n",
        "**[1]**: Gridded Population of the World v4 (https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count-rev11)"
      ],
      "id": "392445d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c00e6a3"
      },
      "outputs": [],
      "source": [],
      "id": "0c00e6a3"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}